<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"><channel><title>The Philosopher Developer</title><description>Dan Tao's blog, The Philosopher Developer</description><link>http://www.philosopherdeveloper.com/</link><item><title>Charts made dead simple with HighTables</title><link>http://www.philosopherdeveloper.com/posts/charts-made-dead-simple-with-hightables.html</link><description><![CDATA[<p>In a footnote to <a href="/posts/making-yaml-safe-again.html">my post a while ago on SafeYAML</a><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, I established a goal of writing more about my many open source projects, which I have a bad habit of not telling anyone about—sometimes even long after they’re finished!</p>

<p>So today I want to write about a pretty good one that’s relatively polished and full-featured. It’s called <a href="http://dtao.github.io/HighTables/">HighTables</a>, and it’s a JavaScript library that makes adding charts to existing sites with HTML tables <em>extremely</em> easy.</p>

<p>Here, I’ll give you an example:</p>

<div class="highlight"><pre><span class="nt">&lt;table</span> <span class="na">class=</span><span class="s">"render-to-bar-chart"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;tr&gt;</span>
    <span class="nt">&lt;th&gt;</span>How easy it is to render a chart<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;th&gt;</span>How easy it is (scale from 1 to 10)<span class="nt">&lt;/th&gt;</span>
  <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;tr&gt;</span>
    <span class="nt">&lt;td&gt;</span>Using Highcharts directly<span class="nt">&lt;/td&gt;</span>
    <span class="nt">&lt;td&gt;</span>3<span class="nt">&lt;/td&gt;</span>
  <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;tr&gt;</span>
    <span class="nt">&lt;td&gt;</span>Using HighTables<span class="nt">&lt;/td&gt;</span>
    <span class="nt">&lt;td&gt;</span>10<span class="nt">&lt;/td&gt;</span>
  <span class="nt">&lt;/tr&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</pre></div>

<p>And the result:</p>
<table class="render-to-bar-chart">
<tr>
<th>How easy it is to render a chart</th>
    <th>How easy it is (scale from 1 to 10)</th>
  </tr>
<tr>
<td>Using Highcharts directly</td>
    <td>3</td>
  </tr>
<tr>
<td>Using HighTables</td>
    <td>10</td>
  </tr>
</table><p>Pretty cool, right?</p>

<p>For what it’s worth, HighTables itself isn’t all that complicated. It leverages the power of two existing libraries, <a href="http://www.highcharts.com/">Highcharts</a> and <a href="http://jquery.com/">jQuery</a>. If you don’t know about Highcharts, it’s a great library that lets you render all kinds of charts (line, area, bar, pie, etc.) in JavaScript. And if you don’t know about jQuery, and you’re a JavaScript developer, you should probably blow the dust off your keyboard and <a href="http://www.doxdesk.com/img/updates/20091116-so-large.gif">ask any JS-related question on the internet</a> to learn more about it.</p>

<p>My problem with Highcharts in the past was that it has <a href="http://api.highcharts.com/highcharts">a <em>behemoth</em> API</a>. Rendering a chart was never a simple matter of calling <code>pieChart()</code>; instead, any chart-creation logic requires a whole ton of options that an average person is very unlikely to remember from one use to the next. And so I always found myself returning to the Highcharts website, perusing documentation, and following examples to ever get a chart to display on sites I’ve worked on.</p>
<figure class="hidden-in-abbreviated-version"><div class="pie-chart" data-title="Value" data-source="#activity-value-vs-time-table" data-value-columns="2"></div>
  <div class="pie-chart" data-title="Time Investment" data-source="#activity-value-vs-time-table" data-value-columns="3"></div>
  <figcaption>Notice any discrepancy?</figcaption></figure><table id="activity-value-vs-time-table" class="hidden">
<thead><tr>
<th>Activity</th>
<th>Relative Value of activity</th>
<th>Hours spent on activity</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: left;">Implementing reporting backend</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">Fiddling with chart rendering</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table><p>Of course, like any good developer, I eventually would get around to writing reusable methods with easier-to-remember interfaces and use <em>those</em> to render my charts. But at some point I noticed a pattern: mostly I was putting charts <strong>where tables already were</strong>. The point of the chart was to display the data in a visual form, to make it a bit easier to parse for a human. The work of translating the data in the table to JavaScript code to render the chart was tedious, but easily repeatable.</p>

<p>And from there came the realization that I could (and probably should) just automate every bit of the process, and write a <em>library</em> to <em>always</em> render a chart from a table—provided that table had some class (like <code>render-to-line-chart</code>) associated with it. So that’s exactly how HighTables works: you add one class to your table, include the library in your page, and charts display automatically. (The charts above are rendered from <a href="javascript:revealTable();">a hidden table</a>, by the way, in case you were confused!)</p>

<p>So, by all means, <a href="https://github.com/dtao/HighTables">try out HighTables</a> and let me know what you think! I’ve already used it a good deal myself, and I can honestly say it’s been really useful to me. But the more use it gets, the more useful it should become as bugs are fixed, features are introduced, yada yada. At least that’s the hope.</p>
<div class="footnotes">
<hr>
<ol><li id="fn:1">
<p>Which <a href="http://rubygems.org/gems/safe_yaml">has come a long way</a>! <a href="#fnref:1" rev="footnote">↩</a></p>
</li></ol>
</div>]]></description><pubDate>Wed, 10 Apr 2013 08:34:00 -0700</pubDate><guid>http://www.philosopherdeveloper.com/posts/charts-made-dead-simple-with-hightables.html</guid></item><item><title>Am I an engineer?</title><link>http://www.philosopherdeveloper.com/posts/am-i-an-engineer.html</link><description><![CDATA[<p>Most people outside of the software development profession (and even many on the inside) may not realize that there is some disagreement within the community as to whether or not software development is a form of <em>engineering</em>.</p>

<p>Let’s say you placed software developers along a spectrum, where at one end “engineering” might as well be a foreign word, and at the other end writing software is <em>obviously</em> a form of engineering, in much the same way that painting or sculpture is obviously a form of art.</p>

<figure class="plain"><img src="/images/engineering-spectrum.png" alt="Is software development a form of engineering?"><figcaption>Is software development a form of engineering?</figcaption></figure>

<p>Personally, I have worked at various points along this spectrum. My first developer job was at a trading company in Philadelphia, which would have fallen pretty close to the extreme left end. At <a href="http://www.thoughtworks.com">ThoughtWorks</a> I would say the culture was somewhere in the middle, with plenty of developers leaning in both directions (I definitely worked with self-professed “engineers” at ThoughtWorks, as well as individuals who vehemently rejected the label).</p>

<p>More recently I’ve worked in environments that fall much closer to the right side of the spectrum. The culture at <a href="http://www.cardpool.com">Cardpool</a>–founded by two former Microsoft employees and with its first two hires from Google—was, not surprisingly, firmly in the software-as-engineering camp. I would expect no different from most large software companies (or developers with large company roots). And a couple of weeks ago I started at Google myself, which I’m guessing will be about as far to the right as I’ll ever get in my career.</p>

<p>(By the way, it’s probably not a coincidence that I chose to assign the left and right sides of the spectrum to the viewpoints that I did; read <a href="https://plus.google.com/110981030061712822816/posts/KaSKeg4vQtz">Steve Yegge’s argument</a> that software developers fall into “liberal” and “conservative” groups to get an interesting view on a related subject. And while you’re at it, also read Richard Gabriel’s <a href="http://www.dreamsongs.com/RiseOfWorseIsBetter.html">The Rise of <em>Worse is Better</em></a> for a much older but very similar perspective on two approaches to software, which he deems the <em>MIT approach</em> versus the <em>New Jersey approach</em>.)</p>

<p>A term that seems to have grown in popularity since I’ve been working is <em>software craftsman</em>. I don’t know that this term is always intended to be in direct opposition to the idea of “software engineering”; but I think it is nonetheless a handy reference point to provide contrast to the perspective that software is definitely engineering.</p>

<p>Which brings me to a disclaimer I probably should have made earlier: I myself lean towards the left on this spectrum, so I am biased. That said, I don’t consider either side to be the “correct” way of viewing software; and in fact, I believe the very attitude that holds one side to be superior to another is probably the most “incorrect” of all possible views on the topic. My primary reason for bringing this up is simply to point out that these different perspectives exist, and to do my best to highlight some important ways in which they are similar, despite their obvious differences.</p>

<p>But before I continue: <strong>what am I even talking about?</strong> I’m sure at least <em>someone</em> reading this has no idea what the terms “engineer” or “craftsman” are supposed to suggest. So let me provide a (relatively) quick, totally oversimplified characterization of each of these ideas.</p>

<h2 id="what_is_an_engineer_1">What is an engineer?</h2>

<figure><img src="/images/engineer-at-work.jpg" alt="An engineer at work"><figcaption>An engineer at work</figcaption></figure>

<p>In <strong>broad, overly general terms</strong>, here’s how I’d describe a software engineer.</p>

<p>Her <strong>code is the product</strong>. She takes pride in its quality and thinks through her designs carefully.</p>

<p>Of course, she wants her code to lead to useful software, which is why it is important that significant thought goes into writing requirements as well. Sometimes an engineer will take ownership of a project’s requirements, but sometimes that will be someone else’s job. If her code is written well and meets requirements, but those requirements turned out to be wrong or poorly defined, an engineer may still feel proud of her work. Her code could still be elegant, even if the product as a whole didn’t turn out to be a success for other reasons.</p>

<p>To an engineer, <strong>the practice of writing software is a discipline</strong>. There is a right way to do it; and although it’s naturally impossible to ever achieve perfection, knowledge and experience equip good engineers to get closer and closer over time, as they learn from experience and apply what they’ve learned to their future work. For this reason an engineer will prefer to formalize her knowledge over time in the form of policies and conventions that can be applied at the team level.</p>

<p>If I may be abstract for a moment, I picture an engineer’s development—not of code, but of herself—as being like the growth of a redwood: up and up, better and better.</p>

<figure><img src="/images/redwood-tree.jpg" alt="An engineer grows like a redwood."><figcaption>An engineer grows like a redwood.</figcaption></figure>

<p>More experience → more knowledge.</p>

<h2 id="what_is_a_craftsman_2">What is a craftsman?</h2>

<figure><img src="/images/craftsman-at-work.jpg" alt="A craftsman at work"><figcaption>A craftsman at work</figcaption></figure>

<p>Just a reminder: I am biased here. But this is roughly what being a “craftsman” means, in my mind.</p>

<p>His <strong>code is a tool</strong>. A tool degrades and becomes blunt over time; this is unavoidable. But naturally, if a tool is important then a craftsman will sharpen it periodically. Tools that are seldom used and have worn out or rusted can simply be replaced or discarded.</p>

<p>A craftsman cares more that his code leads to useful software than that it is elegant or well designed. In fact, if he spent time making his code elegant but the requirements were poorly designed and the resulting software is not useful, a craftsman will be deeply upset because his time was wasted. For this reason he will almost always take at least partial ownership in ensuring that the requirements for a project make sense.</p>

<p>To a craftsman, <strong>writing software is a craft</strong>, based on creativity and technique. There is not a “right” way; there are many ways. Experience alone does not necessarily bring one closer to an ideal (because there is no single ideal). Instead, the more a craftsman learns, the more diverse his set of tools becomes; and his craft is more colorful as a result. A craftsman is less likely than an engineer to adopt formal rules or advocate team-wide policies, since to him code is a relatively personal thing.</p>

<p>I imagine a craftsman’s growth as more like that of baobab tree: out and out, broader and broader.</p>

<figure><img src="/images/baobab-tree.jpg" alt="A craftsman grows like a baobab."><figcaption>A craftsman grows like a baobab.</figcaption></figure>

<p>More experience → more perspective.</p>

<h2 id="where_the_two_shall_meet_3">Where the two shall meet</h2>

<p>As I said, I don’t think either of these approaches is wrong. It’s worth pointing out that both of these mentalities share the same underlying motivation: we all want to build high-quality software. It’s also worth mentioning that even though I’ve painted a contrasting picture of these two types, it isn’t like the things I said about engineers are completely untrue of craftsmen or vice versa. Most engineers certainly care that the software they build ends up doing something useful for someone, and craftsmen naturally enjoy (and even prefer) writing elegant code if they can. The real difference between the two is a matter of <em>relative weight</em> rather than stark acceptance or rejection of principles.</p>

<p>Unfortunately, even though we all may be cut from the same cloth deep down, I tend to feel that it is difficult, and probably inadvisable, to attempt to incorporate both cultures into a single team. Either one or the other view of software development is likely to prevail in a group setting by sheer majority, and those on the other side of the spectrum will not do their best work. Craftsmen in an engineering culture will inevitably feel stifled, while engineers in a craftsmanship culture will become frustrated at what they may perceive as a lack of discipline or consistency.</p>

<p>So if you are in the position of building a software team, it’s worth reflecting for a moment on what the team’s culture currently is (if you’re adding to an existing team) or what you want it to be (if you’re building one from scratch). I’m not saying that if you’re assembling a craftsmanship-style team you should reject all résumés with the words “software engineer” on them; that would be absurd. But as part of the interview process I do think it would be worth your while to try and ascertain where any potential teammates lie on the software-as-engineering spectrum.</p>

<p>Naturally, people are adaptable. Part of the reason I decided to join Google was that I want to learn more about the software engineering culture, and to see what it’s like at a company that has been so successful from it. Just recognize that <em>cultural fit</em> is about more than whether you would be glad to get a beer with someone. I would argue that the worst case scenario<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> for your team would be to bring someone very talented on board who ultimately cannot adjust to your culture. In that case you probably won’t benefit from his or her potential nearly as much as you should, simply because we’re all more productive when working in an environment we can identify with.</p>

<h2 id="does_size_matter_4">Does size matter?</h2>

<p>Among the myriad questions and objections you may find yourself thinking of in response to my position here, I suspect one of them might be: <em>what about large projects?</em> It’s an interesting question: surely you wouldn’t put a team of <em>craftsmen</em> on a large-scale project, would you? Even if we accept that the two approaches to software that I’ve described are both valid in some sense, it would seem that the more structured and organized style of engineers would lend itself better to building large software systems and working together in large groups.</p>

<figure><img src="/images/bridge-under-construction.jpg" alt="Would you trust a bunch of craftsmen with something like this?"><figcaption>Would you trust a bunch of craftsmen with something like this?</figcaption></figure>

<p>I tend to agree with the spirit of this question, but I think there are at least two ways in which it is partially misguided.</p>

<ol>
<li>As I said, people can adapt. This includes craftsman-style developers. Assigned to a large project with a sizable team, I’m confident that any good team—on either side of the spectrum—would adopt a fairly disciplined approach to building the software, with much more structure than perhaps they would on a smaller project.</li>

<li>However, the question implies<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> a certain inevitability to the need for a team to work on a large-scale project. I would challenge this assumption, and return to my earlier point about requirements. I suspect that in many cases, software projects that end up becoming large have little justification for getting that way. Sometimes the size and complexity of a piece of software inflates with the size of the team, forming a harmful feedback loop that leads to bloat. But that’s a topic for another post.</li>
</ol><p>In any case, it is probably true that sometimes large software projects are justifiable<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>; and these require a great deal of organization and collaboration to run smoothly. And so an engineering culture is likely to emerge on these projects, regardless of the sort of software developers working on them. That said, it may sometimes also be possible to break a large project down into smaller projects, in which case semi-autonomous teams might still flourish with a craftsmanship culture.</p>

<h2 id="closing_thoughts_5">Closing thoughts</h2>

<p>I’ve said that it can be difficult for these two kinds of software developers to work together. Regardless, I still believe we have a lot to learn from one another by crossing from one side of the spectrum to the other on a regular basis, at least socially if not professionally. So think about whether you’re more like a “craftsman” or more like an “engineer” as I’ve described them<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>, and seek out people who you think are different from you.</p>

<p>The fact that Richard Gabriel wrote about the MIT approach and the New Jersey approach <em>decades</em> ago—and it still feels relevant today—says something to me. Gabriel may have concluded that “Worse is Better”; but it’s clear to me that, just as there will always be different personalities, both philosophies of software development are here to stay. So let’s do our best to understand each other—or at least not kill each other.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn:1">
<p>OK, maybe not <em>worst</em> case scenario. But it would still be unfortunate. <a href="#fnref:1" rev="footnote">↩</a></p>
</li>
<li id="fn:2">
<p>At least in tone, the way I hear it in my head. <a href="#fnref:2" rev="footnote">↩</a></p>
</li>
<li id="fn:3">
<p>It seems pretty clear to me that Google is a perfect example of this. <a href="#fnref:3" rev="footnote">↩</a></p>
</li>
<li id="fn:4">
<p>I almost worded this, “whether you <em>are</em> a craftsman or an engineer”; but let’s not get caught up in the idea that these terms actually define individuals. It is a convenient mental grouping, for me; but we are all obviously different and how you define yourself may not jive with the division I’ve drawn here at all—in which case, let me know! Leave a comment, or write an e-mail, or just come talk to me some time if we work together. <a href="#fnref:4" rev="footnote">↩</a></p>
</li>
</ol>
</div>]]></description><pubDate>Wed, 03 Apr 2013 13:18:00 -0700</pubDate><guid>http://www.philosopherdeveloper.com/posts/am-i-an-engineer.html</guid></item><item><title>Automating yourself</title><link>http://www.philosopherdeveloper.com/posts/automating-yourself.html</link><description><![CDATA[<p>When my wife and I moved from Philadelphia to San Francisco in 2010, we brought our espresso machine with us.</p>

<figure class="hidden-in-abbreviated-version"><img src="/images/espresso_machine_thumb.jpg" alt="Our espresso machine"><figcaption>Our espresso machine</figcaption></figure>

<p>Back in Philly, we’d had a modest kitchen with just enough counter space for the machine. On lazy weekend mornings, I’d often turn it on and prepare us each a latte drink. It was a nice little ritual.</p>

<p>So we brought it to San Francisco with us. But our first apartment, a little studio we rented from a friend of a friend, didn’t have the room for it. So the machine went into storage.</p>

<p>Then in 2011, we moved to our current apartment in the Mission. The espresso machine is back; but we don’t have quite the counter space that we did in Philadelphia, so it’s sitting on a little cart underneath our microwave, unplugged.</p>

<figure class="hidden-in-abbreviated-version"><img src="/images/espresso_machine2.jpg" alt="Our espresso machine now"><figcaption>Our espresso machine now</figcaption></figure>

<p>This isn’t terribly inconvenient. To use it, I only need to pick it up and set it somewhere—say, on our table—then put it back when I’m finished. Still, the fact remains: <strong>I haven’t used it once</strong><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> since moving here.</p>

<p>How strange, that such a small and seemingly insignificant detail could have such a disproportionate impact on our lives! Instead of plugged in and on the counter, the espresso machine is unplugged and a few feet lower. That’s a small change. But now instead of using the machine every week or so, I <em>never</em> use it. That’s a comparatively <em>big</em> change.</p>

<p>I’ve pondered this from time to time, and an idea that keeps recurring to me is that <strong>we humans are programmable</strong>. At least to an extent. Simply by recognizing that my internal decision-making circuitry seems to ignore the espresso machine when it’s near the ground but will actually commit to using it when it’s on the counter, I can alter my behavior by changing its location.</p>

<p>It’s a silly example, but I think this is actually a really important lesson. The distinction here is essentially the difference between <em>manual</em> and <em>automatic</em> processes. With the espresso machine where it is now, making an espresso is a highly <em>manual</em> process for me—not just the act itself, but even <em>having the idea</em> to make one. Back in Philadelphia, because it was right there in front of me, the idea tended to just pop into my head. Automatically.</p>

<p>We are most effective when we are able to automate the things we do; that much is obvious. But getting software or robots to do all of our grunt work for us isn’t the only kind of automation. The more we can reduce manual deliberation, and say to ourselves less and less, “Remember to do that”–i.e., the more we can automate the way we <em>think</em>–the more we can achieve.</p>

<p>I realize that aside from the espresso machine example this may sound a bit hand-wavy. I intend to address this notion in more concrete terms in a future post; for now I just wanted to get the idea out there.</p>
<div class="footnotes">
<hr>
<ol><li id="fn:1">
<p>That actually isn’t the full story. The real reason I <em>still</em> haven’t used it is that there’s a part that we apparently lost somewhere between Philadelphia and here. But I’ve only included this detail in a footnote because it doesn’t really change my argument; I only even <em>noticed</em> this part was missing after we’d already been in our new apartment for several months. So it’s clear that I am not using the machine nearly as often as I used to. In addition, I’m hoping that by writing this post I will motivate myself to finally order a replacement for that part! <a href="#fnref:1" rev="footnote">↩</a></p>
</li></ol>
</div>]]></description><pubDate>Thu, 28 Feb 2013 06:53:00 -0800</pubDate><guid>http://www.philosopherdeveloper.com/posts/automating-yourself.html</guid></item><item><title>Your brain is a liability</title><link>http://www.philosopherdeveloper.com/posts/your-brain-is-a-liability.html</link><description><![CDATA[<p>It’s natural to think of <em>being smart</em> as an asset. This is obvious in many ways, so I don’t feel I need to enumerate them. But there are also ways that it can be a liability; and since this is the contrarian view, I naturally want to talk about it<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<p>Before I start, though, a note about the word “smart”: it can mean many things. What I am specifically referring to now is what I will call raw <strong>brain power</strong>: the capacity of a person’s mind to think quickly, grasp tricky concepts, store a lot of information at once, and so on. If the mind were a computer, in other words, I’d be talking about <em>hardware</em> (CPU, memory, etc.) as opposed to software.</p>

<p>The <em>software</em> of a computer system makes <em>use</em> of the hardware. It isn’t the other way around. Powerful hardware on its own is useless. For the purpose of this argument I propose that we think of being “smart”–i.e., of having a lot of brain power—as analogous to having a computer with powerful hardware. In contrast, having good instincts, solid judgment, and a fresh perspective—characteristics more in line with what we generally call “wisdom”–is like running well-written software.</p>

<p>With this analogy in mind I think it makes some sense to conceptualize the brain as, quite simply, a <em>tool</em>. And framed in that way, I pose to you the question: if you were a carpenter<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>, and you had a top-of-the-line but rare and expensive tool—I don’t know; let’s say some custom-built <em>screwdriver</em>–would it be a wise idea to build cabinets that <em>required</em> your special tool to repair or disassemble? Or would it be better to build them with regular old screws that any ordinary screwdriver could tighten and loosen?</p>

<p>Maybe you don’t like that analogy. I’ll admit it isn’t perfect. Let’s get more concrete.</p>

<p>It takes being <strong>smart</strong> to be able to read through complex code and understand what it does. I’ve worked with really bright developers who can do this much more quickly and easily than I can. But this is dangerous. If it’s no trouble to <em>you</em> to understand something, you will be less likely to recognize that it is overly complicated and could be simplified—for the benefit of your teammates, and even for you, down the road. In contrast, if you struggle to understand what a section of code does, you will be much more inclined to work on making it <a href="/posts/optimize-for-comprehensibility.html">more comprehensible</a>.</p>

<p>Or suppose your code exhibits some strange behavior. There’s a subtle bug and no one on the team is quite sure what could be causing it. If you’re smart, your brain will scan its data banks of knowledge faster than any of your teammates; and while everyone else is scratching his head you may develop a <em>hunch</em> what the problem might be. But your hunch could be wrong, and you may end up wasting a lot of time exploring the wrong possibility. When you’re generally right about these things, it is all too easy to assume you’re <em>always</em> right. Your slower teammates will be more cautious about jumping to conclusions and may actually find the source of the problem through careful debugging while you’re busy following your intuition.</p>

<p>Being smart can also cause you to develop the tendency to go with your first idea for a design or an implementation, because your first idea is often a good one. But this doesn’t take into account how much your thinking is influenced by environmental factors, or how you might be “in the zone” at certain times and not at others. Meanwhile, your teammates have less confidence in their brains; so they force themselves to think through multiple alternatives before sitting down to write code. This could well lead them to come up with a superior solution to what you had in mind.</p>

<p>Now, I realize that in a sense all these examples may be construed as a form of <em>hubris</em>, or arrogance. So you could perhaps simplify my argument to this: <em>be careful if you’re really smart, because then you’re liable to become arrogant!</em> But the above are real examples (everything I’ve written about so far I’ve observed in real life); and they don’t actually require cocky or condescending personalities. These are traps that anyone who simply <em>is smart</em>–again, by which I mean, <em>has a really fast/powerful brain</em>–can fall into, regardless of attitude towards other people.</p>

<p>That said, your brain can certainly get in the way of your interactions with others as well.</p>

<p>A common saying is that the best way to learn is to teach. I find a lot of truth to this, especially when someone asks me to explain something and I realize I don’t have the firmest grasp of the subject myself. This leads to mutual learning, which is awesome. One problem I see smart people encounter at times is that they overestimate both their knowledge and their ability to explain things. If you’re smart and you find that someone doesn’t understand you, you’re more likely to attribute it to their smaller brain than to your failure to teach. This results in a double failure—you haven’t examined the holes in your own understanding, and your would-be student hasn’t learned anything.</p>

<p>It goes the other way, too. An even worse mistake that smart people make is to confuse others’ difficulty articulating their ideas with the ideas themselves being bad or misinformed. I’ve witnessed this a <em>lot</em>. The problem is that smart people’s brains work too fast; so while a slower-thinking person is stumbling through proposing a (potentially good) idea, a faster-thinking person evaluates it quickly and makes a premature judgment on the evidence available. This is one reason why I believe some smart people are prone to interrupting. Give your poor teammates a chance to sort out their thoughts while they’re talking! That’s how some people work out their ideas: by talking through them. Though you may grow impatient, it’s important to hear others out for this reason. Often I find that the real gem of a good idea comes right at the end of an otherwise inarticulate argument.</p>

<p>But the absolute <em>worst</em> mistake<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> smart people make, in my experience, is the “I already thought of that” fallacy. Smart people—by the very definition of “smart” that I laid out above—have brains that move quickly. This means that they tend to come up with more ideas at a faster rate than most of their peers. And if you have an idea, but you don’t pursue it, then presumably you have <em>reasons</em> for not pursuing it. So when a smart person’s colleague suggests an idea that the smart person already had, the smart person is very likely to dismiss it. <strong>It could still be a good idea.</strong> Just because you thought of something and then moved on does not mean it isn’t worth revisiting. It pains me to think of all the good ideas that have been dismissed, each simply because a smart person thought of it first but never bothered to actually try it.</p>

<p>My purpose in writing this is actually to address you smart folks directly: you know who you are. I’m not saying that you’re cocky, or arrogant, or anything like that. There are times<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> when <em>I’m</em> the one guilty of all the crimes I’ve outlined here. I’m also not denying that your brain is absolutely an asset in many ways—but you already know that. The respect of your team, your online reputation, the constant flood of recruiting messages in your LinkedIn inbox: these things already attest to that. What I am suggesting is that you consider your brain as a <em>tool</em>, with some drawbacks that you should be aware of.</p>

<p>Naturally, all else being equal, I would like a good brain, just as I’d like good PC hardware. But really powerful hardware can mask some of the problems with crappy software. And crappy software needs to be fixed.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn:1">
<p>I should say this right off the bat: these ideas are (clearly) not my own. I would attribute the seed of an idea underlying this article to Bill Schofield, another former ThoughtWorks teammate. Bill was patient enough to work with me on a project during which I believe I was myself guilty of most of the issues I write about here. <a href="#fnref:1" rev="footnote">↩</a></p>
</li>
<li id="fn:2">
<p>I swear one of these days I’ll think of another profession to constantly compare software development to other than carpentry. <a href="#fnref:2" rev="footnote">↩</a></p>
</li>
<li id="fn:3">
<p>I don’t know why I’d make such an extreme claim like this; I guess I’m just in the mood for some hyperbole. <a href="#fnref:3" rev="footnote">↩</a></p>
</li>
<li id="fn:4">
<p>This is generally not true at work, where I am quite possibly the dumbest person in the room. <a href="#fnref:4" rev="footnote">↩</a></p>
</li>
</ol>
</div>]]></description><pubDate>Fri, 08 Feb 2013 22:11:00 -0800</pubDate><guid>http://www.philosopherdeveloper.com/posts/your-brain-is-a-liability.html</guid></item><item><title>Unbreaking DataMapper</title><link>http://www.philosopherdeveloper.com/posts/unbreaking-datamapper.html</link><description><![CDATA[<h2 id="is_datamapper_inherently_broken_1">Is DataMapper inherently broken?</h2>

<p>In a <a href="http://www.drmaciver.com/2010/04/datamapper-is-inherently-broken/">strongly-worded blog post back in 2010</a>, David MacIver asserted that there is a fundamental flaw in <a href="http://datamapper.org/">DataMapper</a>, an ORM library for Ruby. The core of his complaint is<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> that DataMapper’s default API for saving records hides errors, making it difficult to diagnose what went wrong when something fails. This in turn increases the likelihood of defects going unnoticed during development and testing, resulting in buggier software.</p>

<p>Borrowing from MacIver’s post<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>, the below is a boilerplate example of how one might attempt to save a record and report any failures using DataMapper:</p>

<div class="highlight"><pre><span class="n">my_account</span> <span class="o">=</span> <span class="no">Account</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="ss">:name</span> <span class="o">=&gt;</span> <span class="s2">"Jose"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">my_account</span><span class="o">.</span><span class="n">save</span>
  <span class="c1"># my_account is valid and has been saved</span>
<span class="k">else</span>
  <span class="n">my_account</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">e</span><span class="o">|</span>
    <span class="nb">puts</span> <span class="n">e</span>
  <span class="k">end</span>
<span class="k">end</span>
</pre></div>

<p>The above can be pretty annoying to anyone who expects conciseness from an API. Most developers don’t like the idea of having to write several lines of code just to save a record to a database.</p>

<p>Why not wrap the above into a common helper? This still won’t consistently work, as MacIvers points out with the following example:</p>

<div class="highlight"><pre><span class="n">my_account</span> <span class="o">=</span> <span class="no">Account</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="ss">:customer</span> <span class="o">=&gt;</span> <span class="no">Customer</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="ss">:name</span> <span class="o">=&gt;</span> <span class="s2">"jose"</span><span class="p">))</span>
<span class="n">my_account</span><span class="o">.</span><span class="n">save</span>
</pre></div>

<p>In this case, an error could occur when saving <em>either</em> the <code>Account</code> object <em>or</em> the <code>Customer</code> object. And so a general-purpose helper wouldn’t be enough; one would have to write a special helper for every model, accounting for each of that model’s associations, in every application.</p>

<p>I certainly sympathize with MacIver’s frustration.</p>

<h2 id="why_use_datamapper_at_all_then_2">Why use DataMapper at all, then?</h2>

<p>It’s really a shame that such a “fundamental flaw”<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> exists in DataMapper. Otherwise, I find it to be quite a nice ORM, with at least a couple of (admittedly subjective) advantages over the more popular <a href="https://github.com/rails/rails/tree/master/activerecord">ActiveRecord</a>:</p>

<ol>
<li>With DataMapper, your database schema is defined in your models themselves. The ActiveRecord approach uses a separate file to define table schemas (schema.rb), making it sometimes awkward to reason about code in the classes used to interact with those tables. (The existence of gems such as <a href="https://github.com/ctran/annotate_models">annotate</a> attests to this awkwardness.)</li>

<li>DataMapper adopts the philosophy of having a 1:1 mapping between database records and objects in memory. Whereas in ActiveRecord you might have multiple <code>Account</code> instances referencing the same record (with different dirty local states), in DataMapper this is not the case. The upshot is the elimination of an entire category of bugs (<em>what happened to my attributes?</em>).</li>
</ol><h2 id="addressing_the_problem_3">Addressing the problem</h2>

<p>While I understand where MacIver was coming from when he wrote that original post, when I first read it I found myself scratching my head and wondering, <em>Why didn’t he do something about it?</em> This is particularly vexing given that MacIver mentioned having worked with DataMapper for at least “several months” and bemoaned encountering the same flaw “time and time again.” As a software developer, whenever I find myself repeatedly struggling with a tool–<em>especially</em> <a href="https://github.com/datamapper">an open source one</a>–I inevitably end up trying to patch it or otherwise find some way around its (perceived) shortcomings.</p>

<p>It should be noted that, probably at some point after MacIver’s post, DataMapper <em>did</em> introduce <a href="http://datamapper.org/docs/create_and_destroy.html">a <code>raise_on_save_failure</code> option</a> which (obviously) raises exceptions on save failures. However, these exceptions still don’t include any useful information; and it seems <a href="http://datamapper.lighthouseapp.com/projects/20609/tickets/1322-show-objecterrors-when-raise_on_save_failure-is-set">the DataMapper developers aren’t receptive to the idea that they should</a><sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>.</p>

<p>Luckily, it turns out that a solution to this problem isn’t even particularly complicated. It’s true that wrapping the above snippet into a helper in a <em>client application</em> doesn’t solve the problem; but wrapping it in <em>DataMapper</em> does.</p>

<div class="highlight"><pre><span class="k">module</span> <span class="nn">DataMapper</span>
  <span class="k">module</span> <span class="nn">Resource</span>
    <span class="n">alias_method</span> <span class="ss">:save?</span><span class="p">,</span> <span class="ss">:save</span>

    <span class="k">def</span> <span class="nf">save</span>
      <span class="k">return</span> <span class="k">if</span> <span class="nb">self</span><span class="o">.</span><span class="n">save?</span> <span class="o">||</span> <span class="nb">self</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">empty?</span>
      <span class="n">error_message</span> <span class="o">=</span> <span class="nb">self</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">e</span><span class="o">|</span> <span class="s2">"</span><span class="si">#{</span><span class="nb">self</span><span class="o">.</span><span class="n">class</span><span class="si">}</span><span class="s2">: </span><span class="si">#{</span><span class="n">e</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">', '</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span> <span class="p">}</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">"; "</span><span class="p">)</span>
      <span class="k">raise</span> <span class="no">SaveFailureError</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">error_message</span><span class="p">,</span> <span class="nb">self</span><span class="p">)</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>
</pre></div>

<p>How is the above any different from writing a wrapper in your application? Simple: every time a resource is saved in DataMapper, the <code>save</code> method is called (internally). This means that in the simple case—where saving a record fails because it is invalid—the exception raised will be informative by reporting the record’s validation errors. In the more complex case—where saving a record fails because its child is invalid—the exception raised will be informative by reporting the <em>child’s</em> validation errors.</p>

<h2 id="enter_dmnoisyfailures_4">Enter dm-noisy-failures</h2>

<p>I’m sure you saw this one coming from a mile away. Yes, I wrote a gem to do what I’m describing: <a href="http://dtao.github.com/dm-noisy-failures">dm-noisy-failures</a> (the excerpt above is taken directly from the library). This gem overwrites DataMapper’s <code>save</code>, <code>update</code>, <code>create</code>, and <code>destroy</code> methods with variations that throw exceptions (with descriptive error messages) on failure. The original methods returning true and false are aliased as <code>save?</code>, <code>update?</code>, <code>create?</code>, and <code>destroy?</code>–a nice resolution, in my opinion, as it conforms to existing Ruby idioms.</p>

<p><a href="https://github.com/dtao/dm-noisy-failures">Check it out</a> and let me know what you think. <small>My quest to <a href="/posts/making-yaml-safe-again.html">actually publicize my open source projects</a> continues!</small></p>

<p>As <a href="http://www.drmaciver.com/blog">he still seems to be active</a>, and he also seems like a smart and thoughtful guy, I plan on contacting MacIver about my little gem to see what he thinks. It’s very possible he’s not even doing anything with DataMapper anymore; but it can’t hurt to seek his feedback. While I’m at it I should probably also get in touch with the DataMapper folks, who <a href="http://solnic.eu/2012/12/20/datamapper-2-status-and-roadmap.html">are currently working on a major update</a>.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn:1">
<p>To be fair, as the article was written in 2010, MacIver may have changed his stance between then and now. However, I did a brief search for any sort of retraction and couldn’t find one; so I’m sticking with the present tense here. <a href="#fnref:1" rev="footnote">↩</a></p>
</li>
<li id="fn:2">
<p>Which in turn borrows from the <a href="http://datamapper.org/docs/validations">official DataMapper documentation</a>. <a href="#fnref:2" rev="footnote">↩</a></p>
</li>
<li id="fn:3">
<p>I happen to agree with most of MacIver’s points, so I do view DataMapper’s API as flawed. But I’m also quite sure the library’s developers had reasons for designing it that way—or anyway, I haven’t seen any evidence to the contrary—so it’s clearly debatable to some extent. Hence my use of quotes. <a href="#fnref:3" rev="footnote">↩</a></p>
</li>
<li id="fn:4">
<p>As far as I can tell, the DataMapper team’s reasoning for excluding validation errors from exceptions is that “[the] #save command can return false for reasons other than validations being invalid.” This seems to me like an unfortunate case of <a href="http://en.wikipedia.org/wiki/Perfect_is_the_enemy_of_good"><em>the perfect is the enemy of the good</em></a>. <a href="#fnref:4" rev="footnote">↩</a></p>
</li>
</ol>
</div>]]></description><pubDate>Tue, 29 Jan 2013 23:21:00 -0800</pubDate><guid>http://www.philosopherdeveloper.com/posts/unbreaking-datamapper.html</guid></item><item><title>Making YAML safe again</title><link>http://www.philosopherdeveloper.com/posts/making-yaml-safe-again.html</link><description><![CDATA[<p>TL;DR: Check out my new gem, <a href="http://dtao.github.com/safe_yaml">SafeYAML</a>. It lets you parse YAML without exposing your app to security exploits via arbitrary object deserialization.</p>
<hr><p>There was <a href="http://news.ycombinator.com/item?id=5028218">quite a stir in the Rails community recently</a> about a serious security vulnerability in Rails. To be more specific: <a href="https://groups.google.com/forum/#!topic/rubyonrails-security/61bkgvnSGTQ/discussion"><em>every version of Rails</em></a>. We found out about this right away at <a href="http://www.cardpool.com/">Cardpool</a>, in part because Cardpool is a YC company and Paul Graham forwarded an e-mail from Thomas Ptacek to all YC alums warning of the vulnerability pretty much as soon as it was discovered.</p>

<p>Without getting too caught up in the weeds, I will just say the vulnerability was ultimately a consequence of the fact that Ruby’s <a href="http://www.yaml.org/">YAML</a> library by default permits the deserialization of arbitrary Ruby objects. This is a problem for Rails—as well as many other Ruby frameworks, to be fair—because, until patches were released to address this problem, any Rails app could be “tricked” into parsing malicious YAML by basically anybody, without any special credentials. The key weakness in Rails, specifically, was that Rails would automatically parse the parameters of any XML request, including parameters like this:</p>

<div class="highlight"><pre><span class="nt">&lt;data</span> <span class="na">type=</span><span class="s">"yaml"</span><span class="nt">&gt;</span>--- !ruby/object {}<span class="nt">&lt;/data&gt;</span>
</pre></div>

<p>I’m not giving anything away here; exploits <a href="https://community.rapid7.com/community/metasploit/blog/2013/01/09/serialization-mischief-in-ruby-land-cve-2013-0156">have already been made public</a>. The important takeaway is pretty simple: never parse YAML from untrusted user input. Not in an application, and <em>definitely</em> not in a framework. Which means, in the case of Rails, don’t automatically parse params as YAML. The patches that were released (and which we quickly deployed, obviously) addressed this issue by disabling XML parameter parsing by default. (Less aggressive patches were also made available for sites that needed to parse XML params by simply removing YAML from the list of types that could be embedded in an XML request.)</p>

<p>While this is probably fine for 9 out of 10 websites, and probably more than that since so few sites actually have any sort of API that accepts YAML—including Cardpool, I should add—it still felt a little frustrating to me, for a couple of reasons.</p>

<ol>
<li>YAML is, to me at least, a really sweet data format<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</li>

<li>This whole vulnerability nonsense <strong>could have been avoided in the first place</strong> if we were willing to give up one teensy weensy little feature of Ruby’s YAML library: the ability to deserialize arbitrary objects.</li>
</ol><p>Don’t get me wrong; I realize this can be a very useful feature, especially for tools that aim to hide away the details of sharing objects between processes (or entirely different machines). But in the context of a web service or an API, where you have an application and you want to send some information to my service, there’s no reason for you to even know what types <em>exist</em> within my application’s domain, let alone serialize or deserialize them. Strings, numbers, lists and maps are all we really need.</p>

<p>And so with this in mind—and after discovering that <a href="http://pyyaml.org/wiki/PyYAMLDocumentation#Loader">Python’s YAML module has a <code>safe_load</code> method</a>–I <a href="http://stackoverflow.com/questions/14348538/is-there-an-equivalent-to-yaml-safe-load-in-ruby">asked on StackOverflow</a> if there’s any way in Ruby to parse YAML without deserializing arbitrary objects. The answer I got led me to write a library that does precisely that: <a href="http://dtao.github.com/safe_yaml">SafeYAML</a><sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>.</p>

<p>Basically what SafeYAML does is override Ruby’s built-in <code>YAML.load</code> method with an implementation that only deserializes a safe set of types: strings, numbers, arrays, hashes, and a few others. The beauty (in my humble opinion) of this approach is that it makes SafeYAML a great drop-in enhancement to any website that directly or indirectly parses user-supplied YAML. Simply by adding a dependency on the <a href="http://rubygems.org/gems/safe_yaml">gem</a>, without any additional code changes, you can have your cake and eat it too: re-enable YAML parsing in your application, without exposing yourself to a well-known exploit.</p>

<p>Check it out and let me know what you think! Bug reports, pull requests, etc. all welcome <a href="https://github.com/dtao/safe_yaml">on GitHub</a>.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn:1">
<p>Even while I typed that sentence, I felt <em>sure</em> there are articles I could find pretty quickly on Google about why YAML sucks. I choose not to seek out these articles (yet). <a href="#fnref:1" rev="footnote">↩</a></p>
</li>
<li id="fn:2">
<p>As a side note, one of my goals for 2013 (a little late to call it a resolution I suppose) is to actually maintain and publicize the growing number of mostly-open-source projects I’ve started and, in most cases, abandoned over the years. Some are pretty far along; others are still little seedlings; still others exist nowhere but in my brain. But I think a worthwhile aim for this year is to go back through all of these projects and either get back to work on them or scrap them for good. SafeYAML is just the first of many. <a href="#fnref:2" rev="footnote">↩</a></p>
</li>
</ol>
</div>]]></description><pubDate>Thu, 24 Jan 2013 00:00:00 +0000</pubDate><guid>http://www.philosopherdeveloper.com/posts/making-yaml-safe-again.html</guid></item><item><title>A/B testing and irreducible complexity</title><link>http://www.philosopherdeveloper.com/posts/ab-testing-and-irreducible-complexity.html</link><description><![CDATA[<p>I was raised in a devout Christian family, which resulted in a fair amount of inner conflict and soul-searching throughout my academic life, <em>particularly</em> with respect to my ninth-grade education on evolution<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>. This in turn ultimately led me to read a book called <a href="http://www.amazon.com/Darwins-Black-Box-Biochemical-Challenge/dp/0743290313">Darwin’s Black Box</a> by Michael Behe, which argues in favor of <a href="http://en.wikipedia.org/wiki/Intelligent_design">intelligent design</a><sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> on the basis of a concept called <a href="http://en.wikipedia.org/wiki/Irreducible_complexity"><strong>irreducible complexity</strong></a>. It is actually a pretty reasonable argument, in my opinion—though I’m admittedly no expert on the subject—at least in that its premise seems plausible. To summarize in one sentence: Behe argues that there are systems in present-day organisms consisting of interacting parts, each of which on its own would provide no reproductive advantage to an individual and so cannot be explained purely by Darwinian natural selection. Only taken <em>as a whole</em> do these systems provide reproductive advantages; and so some other process must have generated them (where intelligent design enters the picture).</p>

<p>Behe provides plenty of low-level biochemical examples that I won’t bother you with, primarily because I don’t remember them. But whether or not you agree with his argument—and my limited research leads me to believe that (surprise!) most of the scientific community does <strong>not</strong>–I think the <em>concept</em> of irreducible complexity is a useful one. Even if Behe is wrong with respect to evolution, we all know and probably to some extent accept the idea behind <em>the whole is greater than the sum of its parts</em>. Not everything in this world is the end result of some sequence of perfectly incremental changes, each coherent and explicable in its own right. Morever, if a whole could be greater than the sum of its parts, this leaves open the possibility that any given part on its own could even have <em>negative</em> effects, and only contribute towards a positive whole in concert with other parts.</p>

<p>This is a particularly important lesson for software developers—we who are practically hard-wired to test the validity of every assumption and break all problems into smaller pieces. We do love our A/B testing; but as <a href="http://techcrunch.com/2013/01/12/current-conversion-rate-and-desired-confidence-interval-will-help-you-avoid-analysis-paralysis-stop-running-stupid-tests/">Robert J. Moore recently wrote in an article on TechCrunch</a>, these can be taken too far. I have been disheartened on more than one occasion by data-driven minds pushing to validate a large feature through A/B testing each of its smaller parts individually, only to “discover” that the feature had no impact, or even a negative impact, on whatever was being measured. I can’t prove it (without buy-in, that is), but my suspicion is often that the larger feature <em>in its complete form</em> might still have yielded positive results in these cases.</p>

<p>It’s difficult to make this argument, though. The obsessively data-driven approach is actually a very scientific way of tackling a problem: as we all learned in science class, the only true way to test a variable is in isolation, with all other potential factors held constant. One of the problems with applying this scientific methodology to a software project, of course, is that you cannot possibly hold all factors but one constant. The market, your competitors, your users—everything is changing around you at all times. But even if you <em>could</em> somehow contain all that, there remains that nagging possibility that Behe was right, and you risk breaking a big good thing into many small bad things.</p>

<p>How do you draw the line? I’m afraid I don’t have a satisfying answer to that. But from experience, I think I prefer to lean closer to the “test the whole feature” side of the spectrum than the “test each part by itself” side.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn:1">
<p>Not that my parents were Biblical literalists. I never heard either my mom or my dad argue with any passion for a <a href="http://en.wikipedia.org/wiki/Young_earth">Young Earth</a>, for example. I’m inclined to believe my sense of friction between religion and science during my formative years was as much a result of anti-religious sentiments among my science teachers (and peers) as anything else. <a href="#fnref:1" rev="footnote">↩</a></p>
</li>
<li id="fn:2">
<p>Not necessarily of theistic origin. <a href="#fnref:2" rev="footnote">↩</a></p>
</li>
</ol>
</div>]]></description><pubDate>Sun, 13 Jan 2013 00:00:00 +0000</pubDate><guid>http://www.philosopherdeveloper.com/posts/ab-testing-and-irreducible-complexity.html</guid></item><item><title>Optimize for comprehensibility</title><link>http://www.philosopherdeveloper.com/posts/optimize-for-comprehensibility.html</link><description><![CDATA[<h1 id="starting_with_an_outline_1">Starting with an outline</h1>

<p>More than one of my high school English teachers taught us that when you’re writing a paper, you should start by making an outline of your high-level points. This way, they told us, you would have a “skeleton” paper already written, which you could then “flesh out” by filling in appropriate details here and there.</p>

<p>I never much internalized this process of starting off with an outline. I wish I had.</p>

<h1 id="to_design_or_assemble_2">To design or assemble</h1>

<p>My first project at <a href="http://www.thoughtworks.com/">ThoughtWorks</a> was in Dallas, TX. During a car ride back to the office after lunch one day, I was having a conversation with Billy, one of the client company’s developers; and he mentioned that he had recently been to a Google conference to learn about <a href="https://developers.google.com/web-toolkit/">Google Web Toolkit</a> (one of the technologies we were using on the project), among other things. I can’t recall everything we talked about, but something that Billy said during that conversation has stuck with me ever since:</p>

<blockquote>
<p>Companies like Google, Microsoft, Apple—they are the LEGO makers. We are just the assemblers.</p>
</blockquote>

<p>At the time, I strongly disagreed with Billy on this point. Part of me wanted to blurt out, “Speak for yourself!” I didn’t say that, of course; in fact I wasn’t even sure why I felt so strongly in opposition to this statement. Probably more than any other reason, I was just feeling defensive against what I felt was a belittling thing to say about being a software developer.</p>

<p>What I <em>did</em> say was something to the effect that we are all LEGO makers in the sense that we should strive to write clean code, to design clear interfaces, to build reusable components, etc. Billy smiled but clearly didn’t agree with me. In retrospect, he probably thought I was being naïve.</p>

<h1 id="a_lesson_on_refactoring_3">A lesson on refactoring</h1>

<p>On that same project, I became friends with a fellow ThoughtWorks developer, <a href="http://seleniumcapsules.blogspot.com">Yujun Liang</a>. I really enjoyed <a href="http://en.wikipedia.org/wiki/Pair_programming">pairing</a> with Yujun; he and I both understood each other fairly quickly<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, and we got along quite well.</p>

<p>One habit of Yujun’s that bugged me, though, was that he <em>loved</em> to refactor code. In contrast, I had a tendency (at the time) to prefer moving more quickly and building more features. I did understand the value of refactoring in many cases, such as to reduce code duplication. However, Yujun consistently engaged in a particular type of refactoring that I, at the time, found somewhat maddening.</p>

<p>Since a code snippet will probably better illustrate this type of refactoring than I could with words, here’s an example. Suppose we came across this (completely fabricated) code on our project:</p>

<div class="highlight"><pre><span class="kd">public</span> <span class="kt">void</span> <span class="nf">sendNotifications</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">Notification</span><span class="o">&gt;</span> <span class="n">pendingNotifications</span> <span class="o">=</span> <span class="n">getPendingNotifications</span><span class="o">();</span>

    <span class="c1">// Group notifications by recipient.</span>
    <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Notification</span><span class="o">&gt;&gt;</span> <span class="n">notificationsByRecipient</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Notification</span><span class="o">&gt;&gt;();</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">Notification</span> <span class="n">n</span> <span class="o">:</span> <span class="n">pendingNotifications</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">String</span> <span class="n">recipient</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="na">getRecipient</span><span class="o">();</span>
        <span class="k">if</span> <span class="o">(!</span><span class="n">notificationsByRecipient</span><span class="o">.</span><span class="na">containsKey</span><span class="o">(</span><span class="n">recipient</span><span class="o">))</span> <span class="o">{</span>
            <span class="n">notificationsByRecipient</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">recipient</span><span class="o">,</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Notification</span><span class="o">&gt;());</span>
        <span class="o">}</span>
        <span class="n">notificationsByRecipient</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">recipient</span><span class="o">).</span><span class="na">add</span><span class="o">(</span><span class="n">n</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="c1">// For each recipient, create a new notification task to be executed by the scheduler.</span>
    <span class="n">Set</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">recipients</span> <span class="o">=</span> <span class="n">notificationsByRecipient</span><span class="o">.</span><span class="na">keySet</span><span class="o">();</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">String</span> <span class="n">recipient</span> <span class="o">:</span> <span class="n">recipients</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">List</span><span class="o">&lt;</span><span class="n">Notification</span><span class="o">&gt;</span> <span class="n">notificationsForRecipient</span> <span class="o">=</span> <span class="n">notificationsByRecipient</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">recipient</span><span class="o">);</span>
        <span class="n">NotificationTask</span> <span class="n">task</span> <span class="o">=</span> <span class="k">new</span> <span class="n">NotificationTask</span><span class="o">(</span><span class="n">recipient</span><span class="o">);</span>
        <span class="k">for</span> <span class="o">(</span><span class="n">Notification</span> <span class="n">n</span> <span class="o">:</span> <span class="n">notificationsForRecipient</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">task</span><span class="o">.</span><span class="na">addNotification</span><span class="o">(</span><span class="n">n</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="n">task</span><span class="o">.</span><span class="na">scheduleForDelivery</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span>
</pre></div>

<p>Yujun would refactor the above into something like this:</p>

<div class="highlight"><pre><span class="kd">public</span> <span class="kt">void</span> <span class="nf">sendNotifications</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Notification</span><span class="o">&gt;&gt;</span> <span class="n">notificationGroups</span> <span class="o">=</span> <span class="n">groupNotificationsByRecipient</span><span class="o">(</span><span class="n">getPendingNotifications</span><span class="o">());</span>
    <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">NotificationTask</span><span class="o">&gt;</span> <span class="n">tasks</span> <span class="o">=</span> <span class="n">createNotificationTasksFromGroups</span><span class="o">(</span><span class="n">notificationGroups</span><span class="o">);</span>
    <span class="n">scheduleNotificationTasksForDelivery</span><span class="o">(</span><span class="n">tasks</span><span class="o">);</span>
<span class="o">}</span>

<span class="c1">// same implementation as above, just broken up into methods</span>
</pre></div>

<p>Here’s what bothered me about this kind of refactoring at the time: <em>there was still functionality to build.</em> While I would have conceded that Yujun’s refactoring arguably made the code more <em>readable</em>, it did not get us any closer to completing the <a href="http://en.wikipedia.org/wiki/User_stories">stories</a> we needed to finish that iteration. For me, that meant it was not an appropriate use of our time as (expensive) consultants. It was not even reducing code duplication, as the code in question only appeared in one place. I felt this sort of thing belonged at the bottom of a prioritized list of work.</p>

<h1 id="the_limited_value_of_locality_4">The limited value of locality</h1>

<p>You have probably picked up from my careful use of past-tense verbs that my stance on this has changed in the time since that project with Yujun. It has—but not in such a simplistic way that I’d say, “I was wrong, he was right, end of story.” I now think there’s a more nuanced way of looking at how code should read and the value of this style of refactoring. To explain what I mean, let me fast forward to my more recent experience at <a href="http://www.cardpool.com/">Cardpool</a>.</p>

<p>When I first joined the company, every commit I made was code reviewed by a fellow engineer. Very early on—probably within my first couple of weeks—my teammate commented to me that he noticed I tend to write lots of short methods whereas the rest of the engineers on the team generally wrote longer methods. It wasn’t a criticism, just an observation. We agreed that we all have different styles and that there’s an argument that can be made either way: a higher number of shorter methods can lead to more reusable code that’s easier to unit test; whereas favoring fewer, longer methods offers the advantage of greater <strong>locality</strong>: you can see everything the code is doing in one place, without having to navigate back and forth between multiple places in a file (or between multiple files).</p>

<p>I would still say this is true to some extent. But curiously, the longer I’ve been at Cardpool, working in a codebase where this <em>fewer-longer-methods</em> style is the prevailing one, the more I’ve started to doubt one of the supposedly greatest benefits of locality: that it makes code easier to understand. I’ve found that long methods can have the opposite effect: when you have to scroll multiple times the height of your screen to read the entirety of a method’s code, locality falls apart. There’s so much to hold in your brain at once, it becomes difficult to reason about what the code is doing, where changes should be made, and what impact they’ll have.</p>

<p>I think there’s certainly some point at which you can go too far in the opposite direction; i.e., to make the <em>reductio ad absurdum</em> argument, suppose you committed to never writing methods longer than a single line. Reading such code would be like trekking through a treacherous jungle, a veritable nightmare. So, as with most things in life, there’s a balance to be reached. I just happen to believe that the optimal balance is pretty far down towards the “shorter” end of the spectrum.</p>

<h1 id="details_are_irrelevant_yet_responsible_5">Details are irrelevant yet responsible</h1>

<p>I just started reading the book <em>I am a Strange Loop</em> by Douglas Hofstadter (easily one of my favorite authors after reading <a href="http://en.wikipedia.org/wiki/Godel_escher_bach"><em>Gödel, Escher, Bach</em></a>); and in one of the earlier chapters he discusses the notion that <strong>the low-level details of a system are simultaneously <em>responsible</em> for the system functioning yet <em>irrelevant</em> to how the system works</strong>. I think it was while reading this passage that the idea I’m working towards truly started to crystallize for me:</p>

<blockquote>
<p>[L]et us think for a moment about […] a gas in a cylinder with a movable piston. If the gas suddenly heats up (as occurs in any cylinder in your car engine when its spark plug fires), then its pressure suddenly increases and <em>therefore</em> (note the causal word) the piston is suddenly shoved outwards. Thus combustion engines can be built.</p>

<p>What I just told is the story at a gross (thermodynamic) level. Nobody who designs combustion engines worries about the fine-grained level—that of molecules. No engineer tries to figure out the exact trajectories of 1023 molecules banging into each other! The locations and velocities of individual molecules are simply irrelevant. All that matters is that they can be counted on to <em>collectively</em> push the piston out. Indeed, it doesn’t matter whether they are molecules of type X or type Y or type Z—pressure is pressure, and that’s all that matters. The explosion—a high-level event—will do its job in heating the gas, and the gas will do its job in pushing the piston. This high-level description of what happens is the <em>only</em> level of description that is relevant, because all the microdetails could be changed and exactly the same thing (at least from the human engineer’s point of view) would still happen.</p>
</blockquote>

<p>I love this, and I think it covers a concept that is extremely useful for us software engineers. Embrace the notion that the low-level <em>implementation</em> (details) of software, while indisputably <em>responsible</em> for how that software works, can be at the same time irrelevant, extraneous, unimportant—however you want to put it—to understanding the software at a higher level.</p>

<p>Elsewhere in the book, Hofstadter makes this more general point about how much of our understanding of the world actually relies on, essentially, <em>ignoring</em> lower-level details:</p>

<blockquote>
<p>To describe a gas’s behavior by writing a gigantic piece of text having Avogadro’s number of equations in it (assuming such a herculean feat were possible) would not lead to anyone’s understanding of anything. But throwing away huge amounts of information and making a statistical summary could do a lot for comprehensibility. Just as I feel comfortable referring to “a pile of autumn leaves” without specifying the exact shape and orientation and color of each leaf, so I feel comfortable referring to a gas by specifying just its temperature, pressure, and volume, and nothing else.</p>
</blockquote>

<h1 id="the_importance_of_a_consistent_abstraction_6">The importance of a consistent abstraction</h1>

<p>Through experience, reading, and thinking a lot on my own I’ve come to appreciate Yujun’s style of refactoring much more over time. I think I understand better now how important such work is to the sustained health of a codebase. But even though I appreciate the <em>intent</em>, there remains the question of what the end result should be.</p>

<p>A mantra that many of us have heard is that “good code should read like prose”–a quote I want to attribute to <a href="http://en.wikipedia.org/wiki/Donald_Knuth">Donald Knuth</a>, but I could be wrong—and I’m inclined to <em>sort of</em> agree with that. It’s right in line with another mantra we’ve all heard: that when writing code, you should “optimize for readability” (as opposed to, e.g., performance). These are good guidelines, but in my opinion they only get us about halfway to where we should be.</p>

<p>I was fortunate enough to work with <a href="http://blog.thepete.net">Pete Hodgson</a>, another former ThoughtWorks teammate, on multiple projects. On our first project together, Pete took up the burden of teaching me a valuable lesson about how <em>not</em> to write code, in response—to my embarrassment—to a rather clumsy bit of work I had done.</p>

<p>Pete noticed that in one of my commits I had added a snippet of code in a place where, from an organizational standpoint, it simply didn’t belong. <em>Functionally</em> the code did what I intended; but its placement was haphazard, something I hadn’t put any reasonable amount of thought into. I would compare my process for picking its location to spinning a globe and landing your finger on a random spot and declaring: “That’s where I’ll build my house!”</p>

<p>The problem actually went beyond the poor placement of a code snippet within a larger codebase, though. I really can’t recall what the actual code was, so I’ll just write another little fabrication to illustrate the problem:</p>

<div class="highlight"><pre><span class="c1">// what was already there</span>
<span class="nx">updateListContents</span><span class="p">();</span>
<span class="nx">attachEventHandlers</span><span class="p">();</span>
<span class="nx">refreshStyles</span><span class="p">();</span>

<span class="c1">// what I added</span>
<span class="k">for</span> <span class="p">(</span><span class="kd">var</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">pages</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="nx">pageIsHidden</span><span class="p">(</span><span class="nx">pages</span><span class="p">[</span><span class="nx">i</span><span class="p">]))</span> <span class="p">{</span>
    <span class="nx">hideDialogs</span><span class="p">(</span><span class="nx">pages</span><span class="p">[</span><span class="nx">i</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>

<p>Again, that isn’t the actual code I wrote; but I’m pretty confident it was something like that. In retrospect, I’m pretty ashamed to admit that I <em>ever</em> failed to see the problem there. But as Pete was kind enough to articulate for me, and as I quickly understood, there most certainly is a problem: the above code fails to maintain a <strong>consistent abstraction level</strong>.</p>

<p>It was as if Yujun had started to refactor the code, then got pulled away to work on something else. Or, to return to Hofstadter’s point about irrelevant details, it was as though I started to tell you a story by describing some events that occurred and the actions different characters performed, then got to a scene where two characters play a game of chess and suddenly started listing every single move. This has a jarring, botched effect—in code <em>or</em> in prose. Such inconsistency pulls you out from a level of understanding and plunges you “into the weeds”–as apt a metaphor as I’ve heard to describe this sort of thing—by providing <em>detail that is irrelevant to a high-level understanding of what’s important</em>.</p>

<h1 id="what_comprehensible_code_looks_like_7">What comprehensible code looks like</h1>

<p>I mentioned that I only half-agree with the assertion that code should read like good prose. Actually, it might have made sense to say that at the time, whoever first said it. But I would argue that with the advent of <a href="http://en.wikipedia.org/wiki/Hypertext">hypertext</a> we’ve gained an even better way to present information than prose, which is generally linear. The more I think on this idea, the more I like it: <strong>unlike prose, good code should read like Wikipedia</strong>.</p>

<p>Some of you probably instinctively know what I mean by this, but let me explain. When you read an article on Wikipedia (in hypertext), you get a high-level description of some subject—a historical event or figure, a technology, a scientific theory, etc. This description maintains what I’ll call a consistent “altitude”–that is, let’s say, a 10,000-foot view. A well-written article keeps this altitude without diving much deeper; that is left to the reader who wants to find out more about some subtopic of the current subject.</p>

<p>When you <em>do</em> find some part of an article on Wikipedia fascinating, what normally happens (at least with me) is you find yourself clicking on links which take you to more details or otherwise provide greater context on whatever you happen to be reading about. This is a great way to provide information, as it is easy to comprehend—thanks to a consistent altitude—while it also empowers the reader to explore ideas in a way that naturally follows his or her own curiosity. (Incidentally, this also makes Wikipedia a very dangerous place if you can’t afford to waste a lot of time!)</p>

<p>And of course, I could replace the word “altitude” in the above paragraphs with “abstraction level” and suddenly we’d be talking about code. Good code is written like a good Wikipedia article—at a consistently high level of abstraction, so that it can be easily understood, but also in a way that invites <em>exploration</em> to the curious developer who wants to know how this method is implemented, or what dependencies that class has, or how these interfaces fit together.</p>

<p>(As a sort of unfortunate aside, I feel compelled to point out—or admit?–that what I’m saying does require a certain level of sophistication among developers’ tools, just as hypertext requires web browsers offering more functionality than simple text viewers. I’m a huge fan of “lean” editors such as <a href="http://www.sublimetext.com/">Sublime Text</a>, or to a lesser extent <a href="http://www.gnu.org/software/emacs/">emacs</a> or <a href="http://www.vim.org/">vim</a>; but with respect to the point I’m making about code reading like hypertext, I do think it’s worth calling out the advantage of both statically typed languages and beefier IDEs such <a href="http://www.eclipse.org/">Eclipse</a> and <a href="http://www.microsoft.com/visualstudio">Visual Studio</a>.)</p>

<p>To provide one more example: if someone asks you what you do professionally, do you start by going through every task you perform on a daily basis at your job and explaining each one in detail? No, you start with a high-level answer, like ”I’m a software engineer” or ”I’m an investment banker” or ”I’m in sales.” If this person asks you follow-up questions, <em>then</em> you provide more information. The more interest someone shows in what you have to say, the more you can go into detail with the confidence he or she actually cares to hear it. In this sense writing code is similar to telling a story or having a conversation.</p>

<h1 id="optimizing_for_the_right_thing_8">Optimizing for the right thing</h1>

<p>This has really all been a long-winded way of saying: <em>absolutely</em> prefer shorter methods. I’m willing to put a stake in the ground on this one now. The more I write software, and the more I think about it, the more I become convinced that this is a crucial part of writing good code that others can understand. Start with the 10,000-foot view, maintain that altitude, and let the reader (i.e., your teammates) decide when to drill deeper for more detail. In this way your code will be <em>discoverable</em>, like a Wikipedia article, and not just <em>linear</em>, like standard prose.</p>

<p>I should be clear about something at this point. I don’t think that writing long methods makes you a bad engineer, or that writing short methods makes you a good one. I do think I’m right on this issue; but I also know there are other engineers (including some of my current teammates) who would likely disagree with me, yet from whom I still have plenty to learn. Probably more importantly, I can’t even claim to be particularly effective at practicing what I preach, at least at the moment. It’s an opinion that has only recently solidified for me, and one that will require a lot of self-discipline for me to start applying consistently to my work.</p>

<p>But there’s a reason I wrote all this, and it all comes back to what Billy said about being assemblers. I think this is at least partially wrong, because even if we are assemblers in some ways, we are also <em>designers</em>; and any system that is designed well must be comprehensible. That’s why I don’t think it’s enough to just write code that gets the job done, nor do I think that “readability” is the right word to describe how we make code accessible to others. Optimizing for <em>comprehensibility</em> is all about abstraction, or as Hofstadter put it: “throwing away huge amounts of information.” It’s about <em>hiding</em> detail, not revealing it all in one place.</p>
<div class="footnotes">
<hr>
<ol><li id="fn:1">
<p>As anyone who’s seriously worked in a pair programming environment before knows, the importance of clear communication between developers is absolutely critical to their ability to work effectively. I’ve sadly had the experience more than once of working with otherwise talented developers with whom I struggled to communicate; and our productivity inevitably suffered as a result. <a href="#fnref:1" rev="footnote">↩</a></p>
</li></ol>
</div>]]></description><pubDate>Sat, 29 Dec 2012 00:00:00 +0000</pubDate><guid>http://www.philosopherdeveloper.com/posts/optimize-for-comprehensibility.html</guid></item><item><title>The universe is a one-way function</title><link>http://www.philosopherdeveloper.com/posts/universe-is-a-one-way-function.html</link><description><![CDATA[<p>Recently my friend Chuck reminded me of a conversation he and I had ages ago about a company called <a href="http://steorn.com/">Steorn</a>. This is a company that publicly claimed, back in 2007, to have developed an <a href="http://en.wikipedia.org/wiki/Overunity">overunity</a> technology. Chuck chastised me for having persuaded him to take the company seriously; to this day, despite their refusal to back down, they have still not convincingly <a href="http://en.wikipedia.org/wiki/Second_law_of_thermodynamics">broken the second law of thermodynamics</a>.</p>
<iframe width="480" height="360" src="http://www.youtube.com/embed/Xy0UBpagsu8" frameborder="0" allowfullscreen="allowfullscreen"></iframe>
<p>Most of my acquaintances with a modest amount of scientific knowledge, of course, dismissed Steorn from the very start. What the company claims to do violates a known law of physics, they argued; therefore it is impossible; therefore they are either lying or confused. Personally, I never did and probably never will fully sympathize with this attitude. While I agree that Steorn probably do not have what they have claimed (and I certainly have no intention of arguing with the laws of thermodynamics!), I disagree with the premise that we can be <em>so sure</em> of things like this that we are justified in rejecting them immediately.</p>

<p>This is actually the same topic I covered in <a href="http://philosopherdeveloper.wordpress.com/2009/12/19/the-myth-of-the-myth-of-perpetual-motion/">my very first post on this blog</a>. But it’s something I feel quite strongly about, so I’ll probably write about it again, and again, until I’m satisfied I’ve fully covered the topic in the way I want (i.e., never). This time around, I want to relate my skepticism<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> in these sorts of matters to the concept of <em>one-way-functions</em>–a mathematical term that I’ll define later in this post. But first, I’ll start with a simple problem.</p>

<h2 id="recognizing_patterns_1">Recognizing patterns</h2>

<p>Back in the old K–12 days, I remember sometimes taking tests with <em>pattern recognition</em> questions. As a kid, I was always frustrated by these questions because I felt that they generally had no right answer. For example, consider this sequence:</p>

<pre><code>1, 2, 4, ...</code></pre>

<p>What is the next element in the above sequence?</p>

<p>If you say <em>8</em>, you’re most likely thinking that the “pattern” illustrated above involves every value in the sequence doubling the previous value:</p>

<pre><code>1, 2, 4, 8, 16, 32, ...</code></pre>

<p>But there are other possible patterns. For example, we could start with 1, and then add linearly increasing values (+1, +2, +3, etc.):</p>

<pre><code>1, 2, 4, 7, 11, 16, ...</code></pre>

<p>Or the pattern could simply consist of the values <em>1, 2, 4</em> repeated over and over:</p>

<pre><code>1, 2, 4, 1, 2, 4, ...</code></pre>

<p>These are all <em>patterns</em>; and they all start the same way, which means that there is no “right” answer to a question like this. There is, in fact, an infinite number of possible patterns I could imagine that would begin with <em>1, 2, 4</em>, and then proceed in an endless variety of ways. So there are infinitely many equally “right” answers to the question.</p>

<h2 id="the_problem_of_deduction_2">The problem of deduction</h2>

<p>The human ability to recognize patterns and predict outcomes based on those patterns is <em>deduction</em>.</p>

<p>There is a game of deduction that a few of my friends like to play called <em>Zendo</em>.</p>

<figure><img src="/images/zendo.jpeg" alt="Zendo"><figcaption>Zendo</figcaption></figure>

<p>In Zendo, one player—the Master—devises a rule involving the game pieces, which he then illustrates via two examples: one embodying the rule, and one not. These examples are designated <em>true</em> and <em>false</em>. The players then take turns assembling their own game pieces in different formations, asking the master whether or not their formations comply with the master’s rule. Eventually, one or more of the players will <em>deduce</em> the rule by observing a <em>pattern</em> across the examples.</p>

<p>I was recently surprised<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> when, during a game, after the master had given his initial two examples, one of my friends announced that he knew the rule already.</p>

<p>This declaration of having the answer so early in the game seemed absurd to me. And I think you can understand why, from my earlier thoughts on pattern recognition. There was no way my friend already <em>knew</em> the rule at this stage, given that there were many possible rules that would be consistent with the examples given.</p>

<p>Put another way: my friend had not narrowed down the possibilities enough to justify his feeling of certainty, at least to my thinking. He was guilty of <strong>overeager deduction</strong>.</p>

<figure><img src="/images/premature_deduction.jpg" alt="Overeager deduction"><figcaption>Overeager deduction</figcaption></figure>

<p>That said, I can’t really blame him. We humans do this all the time. We <em>have</em> to, because the alternative—i.e., basing our beliefs on logical certainties—is completely impractical. But I’ll get to that. Time to shift gears.</p>

<h2 id="oneway_functions_3">One-way functions</h2>

<p>In mathematics, a <a href="http://en.wikipedia.org/wiki/One-way_function">one-way function</a> is one where computing the <em>output</em> for a given <em>input</em> is easy; but performing the reverse is much harder.</p>

<p>This is in contrast to a <em>two-way</em> function, which is easy to reverse. An example of a two-way function would be multiplication. Given an equation like <em>y = 3x</em>, when we apply the <em>input</em> of 5, we simply multiply 3 × 5 and get the <em>output</em> 15. Likewise, if we know the <em>output</em> is 15, we divide 15 &amp;div; 3 and conclude that the <em>input</em> must have been 5.</p>

<p>An example of a <em>one-way</em> function is <a href="http://en.wikipedia.org/wiki/Prime_factorization">prime factorization</a>. Take some large non-prime number—say, 946,905,102,747. Can you tell me this number’s prime factors, i.e., what prime numbers divide it evenly?</p>

<p>This is not an easy problem to solve efficiently. The only obvious way is to just go through prime numbers, one by one, until you’ve found all the factors. Finding an answer in this way would obviously take a very long time for a human being.</p>

<p>However, it is very <em>easy</em> to <em>verify</em> when I tell you that these are the factors:</p>

<pre><code>27, 101, 419, 857, 967</code></pre>

<p>Even without a calculator, it would not take too long to multiply these numbers together and confirm that they indeed all multiply up to 946,905,102,747.</p>

<p>Here, the function in question might be worded in plain English as: <em>What do you get when you multiply all of these prime numbers together?</em> It is very easy to compute the <em>output</em> of this function given the above factors as <em>input</em>. The reverse might be worded: <em>What prime numbers would you have to multiply together to get this number?</em> From the <em>output</em>, figuring out the <em>input</em> is much harder.</p>

<h3 id="data_encryption_4">Data encryption</h3>

<p>With one-way functions, then, what we effectively have are problems that are difficult to <em>solve</em>, yet whose solutions are easy to <em>verify</em>. It turns out that these are highly useful properties in the context of software security. One-way functions constitute one of the fundamental building blocks of data encryption.</p>

<p>An illustration of this is the way passwords are stored. If you create an account with my website, and I then save your password <em>directly</em> in my database, then a hacker who breaks onto my servers can easily read your password.</p>

<figure><img src="/images/plaintext_password.png" alt="Plaintext password"><figcaption>Plaintext password</figcaption></figure>

<p>But suppose instead of storing your password itself, I store the output of some cryptographic (one-way) function, using your password (and perhaps a <a href="http://en.wikipedia.org/wiki/Password_salting">salt</a>) as input.</p>

<figure><img src="/images/encrypted_password.png" alt="Encrypted password"><figcaption>Encrypted password</figcaption></figure>

<p>Now, even if a hacker breaks into my system and sees your <em>encrypted</em> password, it will be very difficult for him or her to figure out what your password actually <em>is</em>–much in the same way it would be difficult for you to factor the number 946,905,102,747. However, it’s very easy for my system to <em>authenticate</em> you when you enter your <em>real</em> password, just as it is easy for you to multiply the factors of that number together once I tell them to you.</p>

<h3 id="oneway_functions_and_black_boxes_5">One-way functions and black boxes</h3>

<p>It might seem that I’ve veered off topic a bit. How are pattern recognition, Zendo, and one-way functions all related?</p>

<p>I believe the concept of a one-way function is broader than you might think from my initial examples.</p>

<p>In Zendo, we can think of a <em>rule</em> as its own kind of function. The input to this function is a formation of game pieces, and the ouput is either true or false.</p>

<figure><img src="/images/zendo_function.png" alt="Zendo as a one-way function"><figcaption>Zendo as a one-way function</figcaption></figure>

<p>Clearly, this function is itself one-way. Given either <em>true</em> or <em>false</em>–even if you already <em>knew</em> the rule—how could you possibly deduce which formation led to that result?</p>

<p>Of course, that isn’t quite the challenge of the game. In Zendo, the players don’t even know what the function <em>is</em>. This is why in the diagram above I’ve depicted the rule as a <a href="http://en.wikipedia.org/wiki/Black_box">black box</a>.</p>

<p>One way<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> of thinking about <strong>black boxes</strong> is that they are a de facto special case of one-way functions: by convention, they only compute results in one direction; and since their internals are unknown, reversing this process—i.e., working out what the <em>input</em> must have been for a given <em>output</em>–is, in the best case scenario, really a challenge of deducing the inner workings of the box.</p>

<p>I think you can also think of a black box as a kind of “shifted” one-way function. Consider my earlier multiplication example: if we have a function <em>y = 3x</em>, then the <em>forward</em> case (multiplication) takes a value for <em>x</em> as input and produces a value for <em>y</em>, while the <em>reverse</em> case (division) takes a value for <em>y</em> as “output” and <em>deduces</em> a value for <em>x</em>. What is <em>fixed</em> in this example is the function itself.</p>

<p>Remember that in the case of a black box, we don’t know what the function is. What we do know is what both the inputs <em>and</em> the outputs are given our firsthand experience. So we could reframe our puzzle-solving problem as a one-way function where the “input” is actually itself a <em>function</em> (or “rule”, or “law”), and the “output” is a set of known results from applying this function to a known set of inputs. In this case, it is the <em>inputs and outputs</em>–our observations—that are fixed.</p>

<figure><img src="/images/shifted_one_way_function.png" alt="A shifted one-way function"><figcaption>A shifted one-way function</figcaption></figure>

<p>And we’re now back to pattern recognition. What I’ve just described is a shifted one-way function in the same way that Zendo is: the inputs are fixed, and outputs may be observed, but the function itself is unknown. Furthermore, while it is easy to “verify” that a theoretical function does indeed produce the observed outputs for the known, fixed set of inputs, going the other way is near impossible—in the same way that there is no right answer to a pattern recognition problem.</p>

<h2 id="the_sherlock_tendency_or_humans_and_overeager_deduction_6">The Sherlock tendency, or: humans and overeager deduction</h2>

<p>Have you ever noticed that in detective stories, when the brilliant detective protagonist finally cracks the case, he generally reveals the full narrative of what happened to a room full of mesmerized listeners? In this narrative, he paints a vivid picture of how the villain prepared, all of the meticulous steps he took to avoid detection, and all of the little mistakes he made leaving clues that ultimately led the detective to expose him.</p>

<p>Here’s why these stories, though I generally quite like them as entertainment, nonetheless fall short for me in terms of logical satisfaction. They are illustrations of what I will call the “Sherlock tendency”, which is this: as human beings, <strong>once we have identified a <em>plausible explanation</em> for some event, we think we have uncovered the truth</strong>.</p>

<figure><img src="/images/sherlock_tendency.png" alt="The Sherlock tendency"><figcaption>The Sherlock tendency</figcaption></figure>

<p>In the abstract, this is not any different from the “overeager deduction” I accused my friend of in Zendo. It is also the same as observing a sequence such as <em>1, 2, 4</em> and feeling certain at having recognized the underlying pattern.</p>

<p>This is all a <em>plausible explanation</em> really is—a proposal for a function which, when applied to the known, fixed inputs of a situation, is consistent with the observed outputs.</p>

<p>Let me try rewording that. Sometimes, we know certain things that happened at one time, and we also know things that happened at a later time; but we aren’t sure what happened in between.</p>

<figure><img src="/images/unknown_events.png" alt="Unknown events"><figcaption>Unknown events</figcaption></figure>

<p>Any murder mystery is an example of this. The victim was alive at one point; later he was discovered dead. The <em>mystery</em> is what happened to him.</p>

<p>A plausible explanation is an attempt to solve the mystery by proposing what the unknown events above <em>might have been</em> while remaining consistent with the events that <em>did</em> happen. This is <strong>clearly not the same as a certainty</strong>. And yet I seem to observe it over and over again, in practically every aspect of the world we live in: history, economics, politics, etc. As long as we can construct a narrative which is <em>consistent</em> with the experiences of our lives and the information we believe we have about the past, we feel justified in subscribing to all that narrative implies.</p>

<p>It’s an easy trap to fall into, because when an explanation is <em>inconsistent</em> with what we know, we can generally rule it out. I guess our natural instinct is to therefore run with consistency when we find it.</p>

<h2 id="why_are_we_like_this_7">Why are we like this?</h2>

<p>Not that it particularly matters, but I do have a hypothesis as to why we humans tend to think in this way.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>In the very beginning of my explanation of one-way functions, I mentioned that they are hard to solve in one direction. This is fundamentally where the term comes from. And so when we think of the mystery of life as a one-way function, where we know what we’ve experienced and we are compelled to make sense of it, we find ourselves in a predicament. It is <em>very hard</em> to solve this problem. In fact, it may be impossible. So our brains aren’t up to the task.</p>

<p>And yet there is a tremendous advantage to understanding the world—both practically and emotionally. Practically speaking, we’re better able to predict and manipulate our environment when we understand its rules—so a greater capacity for understanding is an advantage. I also think we’re biologically hard-wired to feel rewarded or even euphoric whenever we’re able to solve difficult mental problems, for just this reason. It’s a kind of positive reinforcement, encouraging us to pursue even deeper understanding. (It’s one of the reasons I believe humanity has advanced so far in science and technology.) And so emotionally, I think it makes sense for humans to crave this experience: of figuring things out, of “solving” the mysteries of this world and of our lives.</p>

<figure><img src="/images/eureka.jpg" alt="Eureka!"><figcaption>Eureka!</figcaption></figure>

<h3 id="heuristics_in_software_8">Heuristics in software</h3>

<p>In computer science (and other fields as well) there are classes of very hard problems that cannot—at least <a href="http://www.claymath.org/millennium/P_vs_NP/">not yet</a>–be solved efficiently. One such class of problems is known as <a href="http://en.wikipedia.org/wiki/Np_complete">NP Complete</a>. The <em>NP</em> stands for <em>non-polynomial</em> (time), which basically means that these problems take so long to solve, the time required—as a function of the size of the input—cannot even be expressed by a polynomial expression (e.g., <em>t = n<sup>2</sup></em>).</p>

<p>One of the easier-to-explain<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup> examples of an NP Complete problem is a famous one known as the <a href="http://en.wikipedia.org/wiki/Knapsack_problem">Knapsack Problem</a>, which is this: given some container of finite capacity and a set of items with differing weights (or sizes) and values, find the optimal assortment of items that can be stored in the container.</p>

<figure><img src="/images/knapsack.png" alt="The so-called Knapsack Problem"><figcaption>The so-called Knapsack Problem</figcaption></figure>

<p>I happen to have a bit of firsthand experience with this problem, believe it or not, because at <a href="http://www.cardpool.com/buy-gift-cards">Cardpool</a> I recently implemented a feature that allows customers to specify a total card value they’d like to purchase and then automatically populates their cart from available inventory<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>. This is essentially a special case of the knapsack problem where every item’s “weight” happens to also be its value (in fact, internally we still refer to the feature as the “knapsack” feature).</p>

<p>Why do I bring this up? Well, in building this functionality, I was already aware that it’s really an NP problem. Therefore I knew it wouldn’t be realistic to try to solve with 100% accuracy or correctness; taking on a famously hard computer science problem for a simple convenience feature on a retail website would be a bit overkill. Instead, what we software developers do in situations like this is figure out <a href="http://en.wikipedia.org/wiki/Heuristic">heuristics</a>, or rough solutions that are <em>good enough</em> for practical use.</p>

<p>That’s what I did. I wrote a very basic algorithm—which is absolutely <em>not</em> a complete or strictly “correct” solution—and from my tests, sampling thousands of randomized carts, I found that it generally filled them to about 94% of the desired total value. My team and I agreed that this was good enough, and we moved on.</p>

<h3 id="heuristics_in_the_brain_9">Heuristics in the brain</h3>

<p>This is <strong>what we all do</strong>. This is how our brains work. In this world, full of so many competing ideas and possible explanations for the experiences we have, it is not feasible to comprehensively consider every possibility, with all of its nuances, any more than it is possible to efficiently solve an NP Complete problem. There are just too many ideas and not enough time.</p>

<figure><img src="/images/too_many_ideas.jpg" alt="Too many ideas"><figcaption>Too many ideas</figcaption></figure>

<p>So we exercise our internal <em>heuristics</em>, however we may have formed them over our lifetimes—forged by some mysterious blend of instinct, intuition, education, and so on—and we narrow down the options that our brains subconsciously deem worthy of consideration. The more we refine this ability, the more quickly we’re able to arrive at a final decision; this in turn gives us the emotional reward we’re after and propels us forward.</p>

<figure><img src="/images/just_one_idea.jpg" alt="Just one idea"><figcaption>Just one idea</figcaption></figure>

<p>Some of us are able to do this quite quickly, which can be advantageous even when it doesn’t lead to the truth, or to a “correct” result. We get away with it, I believe, because most of the time this strategy leads us to an understanding that is <em>good enough</em>–just like my knapsack algorithm was good enough, or like Newtownian physics was good enough until Einstein came along, or how if you pick any sufficiently controversial topic chances are you’ll be able to find <a href="http://intelligencesquaredus.org/debates">intelligent, well-reasoned arguments on either side</a>–because the truth is complicated, and it’s not possible for our brains to weigh every available piece of evidence and arrive at complete <em>certainty</em> with respect to such issues (that’s why they’re controversial). They are <em>really tough</em> one-way functions; the best debaters among us simply have the best heuristics, albeit ones that could well have led them in opposite directions.</p>

<h2 id="conclusion_10">Conclusion</h2>

<p>I think it’s important to realize that, at a very fundamental level, we actually <em>know</em> very little. The views we have adopted throughout our lives are informed by plenty of experience, sure; but there is so much universe out there, and our experience covers a relatively miniscule portion it. Our brains adapted to this early in our history, and as a species we developed a knack for using <em>heuristics</em> to form a <em>good enough</em> understanding of the world, so as not to end up paralyzed in deep contemplation our entire lives.</p>

<p>Of course I’m speculating a bit here! But can you blame me? I am after all saying that some form of speculation is all that any of us <em>ever</em> does.</p>

<p>I’ll leave you with one final analogy. Have you ever played Sudoku? It’s a great game, though it can be rather maddening when it’s too hard (my wife got me a book of <a href="http://www.amazon.com/Absolutely-Nasty-Sudoku-Official-Puzzle/dp/1402743963"><em>Nasty Sudoku</em></a> and it kills me). Have you ever had that <em>sinking feeling</em> when you’re most of the way through a Sudoku puzzle and suddenly you realize you’ve hit a contradition—there must have been a mistake (probably on your part)?</p>

<figure><img src="/images/sudoku.jpg" alt="Sudoku"><figcaption>Sudoku</figcaption></figure>

<p>This can be very easy to miss when it happens, especially if it occurs early in the puzzle. With so much unknown, a slip-up or illogical move may well not result in any obvious contradictions.</p>

<p>Now think of life as a Sudoku puzzle, but obviously much larger—with a grid extending as far as the eye can see in any direction: hundreds, thousands, millions of squares. You’re <em>never</em> going to fill in the whole grid; that would be tantamount to fully understanding the entire universe. But you’ll still make gradual progress, coming to understand more and more with age. However, you’ll also make mistakes once in a while, even if you’re good; and these mistakes will eventually lead to <em>more</em> mistakes, as happens in Sudoku. If you’re honest with yourself, you’ll therefore have to accept two things: that some of the grid you’ve already filled in is wrong (though you hope it’s a small fraction), and thus if you ever compare your grid to someone else’s and notice a discrepancy, you must be open to the possibility that theirs could be right.</p>

<p>And so the next time someone insists to me that Steorn cannot <em>possibly</em> have achieved overunity, or my friend claims to have <em>immediately</em> identified a rule in Zendo, or I see a detective movie with a resolution that neatly ties up all the details of the crime, I will remain skeptical. The universe is a one-way function; and while it may be easy for us to recognize when ideas are <em>plausible</em>, it is much, much harder to ever find the truth.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn:1">
<p>Ironically, with Steorn, I’d argue that <em>disbelief</em> is the default attitude and therefore those who dismiss the company outright are not really skeptics. Rather, those like myself who disagree (or at least did originally) that the company can be <em>immediately dismissed</em> are the skeptical ones—i.e., the ones skeptical of others’ certainty. <a href="#fnref:1" rev="footnote">↩</a></p>
</li>
<li id="fn:2">
<p>To be clear: my friend believed that he understood the master <em>personally</em> so well that he had a strong intuitive sense of the sort of rules he would think of. I remain skeptical. <a href="#fnref:2" rev="footnote">↩</a></p>
</li>
<li id="fn:3">
<p>No pun intended—I swear! <a href="#fnref:3" rev="footnote">↩</a></p>
</li>
<li id="fn:4">
<p>Notice that this hypothesis is itself a demonstration of the Sherlock tendency! Recursion, anyone? <a href="#fnref:4" rev="footnote">↩</a></p>
</li>
<li id="fn:5">
<p>By which I mean, the only example I know of off the top of my head besides <a href="http://en.wikipedia.org/wiki/Traveling_salesman_problem">Traveling Salesman</a>. <a href="#fnref:5" rev="footnote">↩</a></p>
</li>
<li id="fn:6">
<p>If you’d like to see this feature in action, <a href="http://www.cardpool.com/buy/home-depot-gift-cards">try using the “Bulk Purchase” button on Cardpool’s Home Depot page</a> and searching for, say, $2000 of cards. <a href="#fnref:6" rev="footnote">↩</a></p>
</li>
</ol>
</div>]]></description><pubDate>Wed, 3 Oct 2012 00:00:00 +0000</pubDate><guid>http://www.philosopherdeveloper.com/posts/universe-is-a-one-way-function.html</guid></item><item><title>Cannibalizing yourself on purpose</title><link>http://www.philosopherdeveloper.com/posts/cannibalizing-yourself-on-purpose.html</link><description><![CDATA[<p>I read an article in the New York Times recently entitled <a href="http://www.nytimes.com/2012/09/22/opinion/nocera-has-apple-peaked.html">Has Apple Peaked?</a> and found myself nodding my head to a lot of the author’s points. The basic premise of the article was this: <em>maybe</em> Apple has peaked, and maybe it isn’t because Steve Jobs has passed away but rather because, as a company on top of the world, they now have everything to lose and can no longer take big risks.</p>

<p>I think there’s something to this, and I’d add another source of inertia for consideration: <strong>hubris</strong> (big surprise to those of you familiar with my general dislike for Apple, I’m sure!). At Apple’s scale, given the massive success they’ve enjoyed over the past several years, I have no doubt that the company’s sense of self-importance is extraordinarily high. Which is obviously justified to a significant degree. But one common observation I have about human nature—and I am increasingly convinced that it applies to businesses the same way it applies to individuals—is that it is very easy to pat yourself on the back for a job well done and claim more credit for your success than you <em>really</em> deserve. Put another way: it can be very easy to miss what an important role <em>luck</em> has played in your life, and thus to take full credit for your good fortune.</p>

<p>Which means that if you’re a company like Apple, you start to get complacent. You watch every product you release hit record-breaking sales, you see your competitors struggling to gain any kind of momentum, you notice the lines stretching for blocks outside every one of your retail stores, and… you start to relax. You think, <em>no one can touch us right now</em>. You feel invincible. You take for granted that everything you do is ground-breaking. Of course it is—how could it not be? You’re Apple!</p>

<p>But we’ve seen this before. Isn’t that how Microsoft felt for much of the past couple decades? And IBM before them? Now Microsoft, despite being a huge and profitable company, is nonetheless considered an underdog in some respects. And whether or not they see themselves that way, I guarantee they <em>feel</em> the pressure of public perception weighing on them. And so for the first time in a long time, Microsoft is now the one taking a big risk—with Windows 8. And you see Nokia in a similar position, taking a big risk with their Lumia phones now that their North American presence has eroded.</p>

<p>All of this has got me thinking: is this the inevitable trajectory of a successful company, or is there a way to mitigate this pattern of risk-taking followed by complacency?</p>

<p>An idea I’ve had for some time now is that of <strong>deliberate self-cannibalization as a strategy</strong>. I wonder if this is an established concept and one that many businesses have tried. Perhaps it’s an age-old idea that was debunked long ago; I don’t know. But the basic concept is this: if you’re Apple today, or Microsoft ten years ago, why not <strong>become your own competitor</strong>, and develop products that take on your existing ones?</p>

<p>Here’s my thinking. While I can completely understand that in a perfect world (for your company), your product would be perfect and gain 100% market share and nobody else could ever chip away at that, you <em>know</em> that will never be true. In fact, the bigger and more dominant you are, the more motivated emerging young entrepreneurs will be to take you on and, if they’re lucky, bring you down. So competition is a given.</p>

<p>The traditional approach for dealing with this seems to be, as far as I can tell, taking one of two paths:</p>

<ol>
<li>
<strong>Litigation</strong>: sue, or threaten to sue, any up-and-coming players and overpower them with your massive legal team before they’re big enough to stand a chance in court.</li>

<li>
<strong>Acquisition</strong>: just buy them up so you can control what they do.</li>
</ol><p>In either case, the sad reality is that it rarely seems the larger company’s intention is actually to leverage any of the smaller company’s innovation. I’ve only ever seen litigation used as a means to protect existing interests. (This is opposed to, say, a company defending a legitimately innovative idea that they still need time to develop before copycats with deeper pockets can come along and beat them to market. I’ve seriously never heard of that, at least in software.) And nine times out of ten<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, it seems acquisition is followed by either total dissolution of the acquired company (<em>Whew! Defused that bomb!</em>) or something spectacularly boring, like integration of the purchased product into the parent’s larger product suite.</p>

<p>Now consider what I’m suggesting as an alternative. I don’t want to pick exclusively on Apple and Microsoft, so let’s talk about somebody else: Google.</p>

<p>Let’s say I’m Google. I have this massively profitable product: AdWords, which relies primarily on my search engine. Then there are a number of other products that are considered core products to my company: Android, YouTube, Gmail, Google Maps, Google+ (I don’t know, I’m just giving examples—let’s say that’s a realistic list). Maybe they’re not all making money, but they’re all major players in their respective markets, all of which are either already huge or growing, or at least have great potential.</p>

<p>What else do I have? Some 30,000 employees, including a significant share of the very best engineers in the world. Also, <em>millions</em> of man hours of hard-earned knowledge and understanding about the types of products we make, the technical challenges associated with them, the needs and wants of the people who use them, and the unspoken rules and tribal knowledge of the industry we’re a part of. Oh yeah, and billions of dollars in cash to fund whatever crazy ideas I want.</p>

<p>These are all things any startup or new competitor is not going to have—at least not to the extent that I do. So why not assemble a new team within Google, consisting of employees who have proven their value in any number of ways and are itching to work on something new or otherwise break out a bit from the existing structure of the company, and ask them to work on… a totally different take on search? Or a completely new format for sharing video content? Or any number of other products that <strong>I know are going to come along anyway, and so I might as well build them myself</strong>?</p>

<p>It seems to me the traditional thinking here is that this would be bad because it would eat into existing business. But to me, turning a blind eye to the <strike>possibility</strike> inevitability that someone else is going to come after my core business—aggressively—is delusional. And expecting that just because I’m the best at what I do now, no one else will ever be able to beat me at my own game… well, that strikes me as wishful thinking.</p>

<p>Of course, I’m not suggesting that it makes sense to do this in all cases, nor that it should necessarily be one of your biggest investments if you’re a large company with many successful products. My intuitive feeling is that something like what I’m describing would make the most sense framed in the following way: whatever you are most fearful of, <strong>whatever you see as the biggest potential threat to your business: <em>build it yourself</em>.</strong></p>

<p>I can’t remember where, but I recently read an article suggesting that Google’s greatest fear right now is Amazon. The author’s reasoning was that Google makes the majority of their revenue through advertising on the kind of searches that lead directly to purchases; i.e., Google’s most profitable searches are for things like shoes and handbags and laptops and tablets, because those are the ones that allow them to display sponsored links to retailers who directly benefit from clicks and thus are willing to pay the most. Which means, basically, that Google relies on users searching through Google for things that they <em>could</em> be searching for on <em>Amazon</em>. Which in turn means that Amazon stands to gain quite a lot by improving their search capabilities and getting aggressive about taking on Google in that arena.</p>

<p>I actually don’t know whether or not this is true (even if it is I’m sure it’s only half the picture), but let’s assume for argument’s sake that it is. In that case, then what I would suggest is that Google start asking themselves questions like: <em>What is it that we’re afraid Amazon might make?</em> And: <em>What would that product look like?</em> And finally: <em>How can we build it before them?</em></p>

<p>Lest you think I’m just talking about Google Shopping—a product that has already existed for quite some time—let me clarify something. I actually think one of the most important aspects of this idea of self-cannibalization is that it should <em>not</em> be treated as a new component of an existing product suite or brand. Remember, my suggestion is to build the thing you’re <em>afraid</em> of. And Google is certainly not afraid of a product called “Google Shopping” (or “Froogle”, or whatever else it used to be called).</p>

<p>So I am talking about creating an entirely new brand, for one thing, and even possibly a whole new company. In fact I’d expect that distancing this new venture publicly from the original company as much as possible would be wise. But the crucial differentiator from any other startup would be that, internally, this project would have access to the same resources, the same engineering know-how, the same marketing muscle, and—perhaps most importantly—the same <em>data</em> as its parent. And it would be run by a team with the incredible advantage of knowing exactly what their “competitor” feared.</p>

<p>Meanwhile, <em>of course</em> you’d still be working as hard as ever on your core product, the one that’s now under friendly fire—just like you would be if an <em>actual</em> competitor had come on the scene and started chipping away at you. But this way, there are two equally good potential outcomes. One is that the core product comes out victorious, and your new internal “competitor” product is either scrapped or consumed for its most worthwhile ideas. The other is that the competitor actually does start eating away at the main business, in which case:</p>

<ul>
<li>That’s exactly what you feared, right? So, that’s bad? Except:</li>

<li>The revenue is actually coming to you anyway. So maybe it’s OK. Plus:</li>

<li>Since both products are yours, you don’t need to become embroiled in a price war. <strong>And</strong>:</li>

<li>If a legitimate outside competitor does come along, now they’ve got to beat <em>two</em> major players—who are working together!–instead of one.</li>
</ul><p>OK, so now that I’ve written at length about this idea, I acknowledge that there are some obvious problems with it. (And probably a lot more problems than I can even think of.) The most obvious is simply that what I’m talking about is, in theory, hugely inefficient. I’m sure there aren’t a lot of executives out there who’d be thrilled at the idea of basically throwing twice the resources at the same problem in anticipation of a nonexistent competitor.</p>

<p>But I see the issue a bit differently. In my experience, doubling the resources invested in a product falls far short of doubling the output (ever heard of the <a href="http://www.amazon.com/The-Mythical-Man-Month-Engineering-Anniversary/dp/0201835959">Mythical Man Month</a>?); and so the perceived “inefficiency” of duplicating effort on a separate project is likely to be seriously exaggerated. In fact, it could very well be more efficient to create a new <a href="http://www.zurb.com/word/two-pizza-team">two-pizza team</a> and put them on a new project, where each individual can make an enormous contribution, than it would be to assign those same individuals to an existing 40-person project, where the communication and coordination overhead of managing such a large team would reduce the proportional effectiveness of each individual and weigh the whole thing down.</p>

<p>Another objection I could easily see to the self-cannibalization idea is that it would dilute the brand and/or fragment the company in a harmful way. This seems like a legitimate danger to me, and one that I believe it would take great care and good judgment to defend against. On some level I suppose a company’s vulnerability to this threat depends on culture, morale, and public perception. But I’d still argue that, in some cases, it’s a challenge worth tackling. Especially when the alternative is resting on your laurels while others are plotting ways to disrupt your business.</p>

<p>Obviously I am no expert on any of this. It’s just an idea. I will close, though, by making just a few sweeping generalizations.</p>

<p>Innovation requires risk. Dominance in technology relies <em>at least in part</em> on innovation, which means that you cannot be a technology company and not expect to take risks. If you are successful, and you want to continue to be successful, but you are no longer willing to take risks, it is only natural that others <em>will</em> be; and for some non-zero fraction of them, those risks will likely pay off. So in the end, I think what I’ve described as “self-cannibalization” is really just smart risk management, though perhaps I could have done a better job defining it.</p>

<p>Anyway, it’s a thought.</p>
<div class="footnotes">
<hr>
<ol><li id="fn:1">
<p>I did not research this figure. <a href="#fnref:1" rev="footnote">↩</a></p>
</li></ol>
</div>]]></description><pubDate>Mon, 24 Sep 2012 00:00:00 +0000</pubDate><guid>http://www.philosopherdeveloper.com/posts/cannibalizing-yourself-on-purpose.html</guid></item></channel></rss>