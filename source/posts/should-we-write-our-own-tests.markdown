---
title: Should we write our own tests?
date: 2014-05-12
published: false
---

I have a feeling anyone reading this is likely to feel that the answer to the question *Should we write our own tests?* is obvious. But obviously what? Yes or no?

A confession, which will probably (and understandably) be offensive to anyone reading who identifies as a QA, test engineer, or something along those lines. (As a weak defense against accusations of being a jerk, I should point out that my first job out of college was as a software QA.) For some time now I've held the belief that we as software developers should definitely be writing our own tests, and so the role of a software QA is one that shouldn't have to exist. To be clear, I have met plenty of very smart and capable QAs and I never questioned the value of their work. I guess you could say I thought of software QA teams as sort of a necessary evil, a hack to provide better quality assurance given that many devs simply *don't* write tests, or write crappy tests.

I also thought of QA folks in the context of big waterfall environments where software projects would go through a development phase, then a QA phase, then a release phase, and so on. This is in contrast to the more "agile" style of iterative development, where the code is built and tested, continuously, either in parallel (two teams) or as one in the same process (devs writing their own tests). So until recently if you had put a gun to my head and asked, *"What do you think of the state of software QA today?!"* I would have nervously blurted, *"I think maybe it's a symptom of a flawed and outdated methodology!"*

Now I'm not so sure. In particular I was moved to think about this question recently as it runs parallel to the issue of validating scientific research. If some group of scientists comes up with a hypothesis, and they conduct an experiment to test the hypothesis, and their experiment shows positive results, great. But they're not going to persuade the scientific community at large of their findings until that research is peer-reviewed, validated by other groups of researchers *without any bias* as to the correctness of the original hypothesis.

Part of the reason peer reviews are so important in science is simply that we can be more confident that empirical data is reliable when we have larger data samples. There's always the possibility that a set of experimental results was a fluke, or (probably more likely) was affected by some external factor that the researchers didn't think of. Another part, though, is that when you have a vested interest in something it can be hard if not impossible to make an objective judgment about it. You might *really* want your hypothesis to be true because it would bring you a lot of prestige to have discovered it; or it might provide support for a political or religious belief you hold; or it might simply be your baby and you've spent so many years on it that it would crush you if it turned out to be wrong.

I think this second reason provides a pretty strong argument in favor of having someone *other than* the dev who wrote some code write the tests for it. Whether that person is a QA or another dev or an SET[^qa-dev-or-set] and whether this happens as part of an iterative process or during a more waterfall-style QA phase are separate questions. The key point is that there should be someone writing tests who is unbiased, who does not have a vested interest in the code passing (or failing) tests, and who has no sentimental attachment to the code. (If you're not a programmer you probably find that last clause to be ridiculous; if you are a programmer I bet you find it to be perfectly reasonable.)

This isn't to say that developers shouldn't *also* write tests for their code, any more than that researchers shouldn't (obviously) test their own hypotheses. I guess it's more a question of where we should set the bar for deeming some developer's work to be acceptable. I still hold the opinion that if a dev doesn't write tests *at all*, that is a pretty bad sign, just as it would be irresponsible for a researcher to promote a hypothesis he himself has never bothered to test. But it doesn't follow that *only* devs should write tests, or that tests written by a dev are necessarily sufficient.

I'm really just thinking out loud here; I'm not saying I've come to the conclusion that it is always necessary for someone other than a developer to write tests. I just suddently find myself much more sympathetic to that view than I used to be, and---as always---I'm finding there's more to the story than I originally thought.

[^qa-dev-or-set]: *Software Engineer in Test*: an official role at Google, as opposed to say a SWE (software engineer) or an SRE (site reliability engineer).
