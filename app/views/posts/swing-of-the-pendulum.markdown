For a while now I've been vaguely troubled by the generally-unquestioned use of the term "flip-flopper" to describe politicians who change their stances on issues. Of course I understand the reason behind the sentiment: we don't want leaders who opportunistically alter their platform just to garner votes (implying that they don't actually believe in the principles they espouse). But at the same time, it seems to me we must allow at least *some* leniency in this area, unless we're truly ready to commit to the notion that people should never change their minds. Which, to me, sounds not all that different from saying that we shouldn't admit when we're wrong.

But I actually have nothing else to say about politics, specifically. That was just a lead-in to a more general point I wanted to make about changing your mind: namely, that we all do it, and that pretending otherwise is destructive and leads to needless conflict.

This recently became top-of-mind for me as I was reading the recent article by Ron Jeffries, [Estimation: The Best We Can Do](http://pragprog.com/magazines/2013-04/estimation), in which Jeffries effectively reverses[^reverses-probably-not-the-right-word] his position on estimation for agile software projects. This is just a couple of months after his article [Estimation is Evil: Overcoming the Estimation Obsession](http://pragprog.com/magazines/2013-02/estimation-is-evil). It reminded me of a similar reversal I saw in grad school, from a prominent figure in the software world (can't remember who, sadly) changing his mind on the subject of software metrics. This guy had written an article advocating for extensive use of metrics over every conceivable part of the software process, only to backtrack a year or so later and concede that you can certainly go too far with that.

Personally, I find this to be a useful guideline, for any position you may find yourself leaning towards: **just assume you're going to change your mind later**. It doesn't mean you don't have perfectly valid reasons right now. But simply that you have valid reasons for thinking something doesn't mean it's right. And even if it *is* right in one situation, that doesn't make it right in every situation. We software developers are ridiculously susceptible to this trap: over-generalizing problems, thinking we can identify The Solution and apply it henceforth to all future scenarios. But then we get a little more experience, we learn that we overlooked some things, the world changes, etc.

Even if you can't really fathom that you'll change your mind about something (example: what if I said to you, *Software shouldn't be tested*?), entertaining the assumption that you will can be a helpful mental exercise. Ask yourself: what *could* cause me to change my mind on this? What would I have to see? What if one of the most respected figures in my life came along and made a compelling argument against what I take for granted?

[^reverses-probably-not-the-right-word]: comment goes here

- always thought public too quick to accuse politicians of being "flip floppers"
  - understand the concern (don't want opportunistic politicians)
  - yet, mentality is that individuals should not change minds?
- useful guideline: assume you'll change your mind later
  - mental exercise: what could happen that would cause me to reverse this position?
  - what argument could someone make that would contradict my basis for this position?
- examples from the software world:
  - ron jeffries, different opinion on estimation: http://pragprog.com/magazines/2013-04/estimation
  - software metrics guy
- corollary: even from people you trust &/or respect, assume *they'll* change their minds later
- never sacrifice judgment for sake of expedience
